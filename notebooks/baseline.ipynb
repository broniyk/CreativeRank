{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d85fa04",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e61557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/broniy/Desktop/CreativeRank/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, CatBoostRanker, Pool\n",
    "from typing import List\n",
    "from settings import DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f61cf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_df = pd.read_csv(\n",
    "    DATA_FOLDER / \"processed/clicked.csv\"\n",
    ")\n",
    "non_clicked_df = pd.read_csv(\n",
    "    DATA_FOLDER / \"processed/non_clicked_large.csv\"\n",
    ")\n",
    "\n",
    "variations_df = pd.read_csv(\n",
    "    DATA_FOLDER / \"processed/feats_df.csv\"\n",
    ").rename(columns={\"id\": \"VARIATION_ID\"}).fillna(\"UNK\")\n",
    "variations_df = variations_df[~variations_df['error'].isna()].drop(columns=['error'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646cd390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_df size before removing small experiments: 501008 rows\n",
      "users_df size after removing small experiments: 500953 rows\n"
     ]
    }
   ],
   "source": [
    "users_df = (\n",
    "    pd.concat([clicked_df, non_clicked_df], axis=0)\n",
    "    .assign(\n",
    "        CLICK=lambda x: (x[\"CLICK_COUNT\"] > 0).astype(int),\n",
    "        EXPERIMENT_DATE=lambda x: pd.to_datetime(\n",
    "            {\n",
    "                \"year\": 2025,\n",
    "                \"month\": x[\"MONTH\"],\n",
    "                \"day\": x[\"DAY\"],\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "    .dropna(subset=[\"CLICK_COUNT\"])\n",
    "    .drop(columns=[\"RN\"])\n",
    "    .fillna(\n",
    "        value={\n",
    "            \"TOTAL_ORDERS_VALUE\": 0,\n",
    "            \"AVG_ORDER_VALUE\": 0,\n",
    "            \"LAST_ORDER_VALUE\": 0,\n",
    "            \"COUNTRY\": \"UNK\",\n",
    "            \"REGION\": \"UNK\",\n",
    "            \"LATEST_CLICK_CLIENT_TYPE\": \"UNK\",\n",
    "            \"LATEST_CLICK_CLIENT_NAME\": \"UNK\",\n",
    "            \"LATEST_CLICK_CLIENT_OS_FAMILY\": \"UNK\",\n",
    "            \"FIRST_UTM_SOURCE\": \"UNK\",\n",
    "            \"FIRST_UTM_CONTENT\": \"UNK\",\n",
    "            \"FIRST_UTM_CAMPAIGN\": \"UNK\",\n",
    "            \"LAST_UTM_SOURCE\": 'UNK', \"LAST_UTM_CONTENT\": 'UNK', \"LAST_UTM_CAMPAIGN\": 'UNK',\n",
    "            \"CITY\": \"UNK\",\n",
    "            \"TIMEZONE\": \"UNK\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# Convert FIRST_ACTIVE_TS to datetime\n",
    "users_df[\"FIRST_ACTIVE_TS_dt\"] = pd.to_datetime(users_df[\"FIRST_ACTIVE_TS\"])\n",
    "\n",
    "# Compute months between today and FIRST_ACTIVE_TS\n",
    "today = pd.Timestamp(datetime.today())\n",
    "\n",
    "# Compute years and months difference and convert to total months\n",
    "users_df[\"MONTHS_SINCE_FIRST_ACTIVE\"] = (\n",
    "    today.year - users_df[\"FIRST_ACTIVE_TS_dt\"].dt.year\n",
    ") * 12 + (today.month - users_df[\"FIRST_ACTIVE_TS_dt\"].dt.month)\n",
    "\n",
    "\n",
    "users_df = users_df[users_df[\"VARIATION_ID\"].isin(variations_df[\"VARIATION_ID\"])]\n",
    "users_df = users_df.drop_duplicates()\n",
    "\n",
    "# Print the size of users_df before removal\n",
    "print(f\"users_df size before removing small experiments: {users_df.shape[0]} rows\")\n",
    "# Remove experiments with less than 100 participants\n",
    "experiment_counts = users_df.groupby(\"EXPERIMENT_ID\")[\"RECIPIENT_ID\"].nunique()\n",
    "valid_experiments = experiment_counts[experiment_counts >= 100].index\n",
    "users_df = users_df[users_df[\"EXPERIMENT_ID\"].isin(valid_experiments)]\n",
    "# Print the size of users_df after removal\n",
    "print(f\"users_df size after removing small experiments: {users_df.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4295346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">CLICK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXPERIMENT_ID</th>\n",
       "      <th>EXPERIMENT_DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>002deaf7-331f-4b5e-866b-f6dad60e4a79</th>\n",
       "      <th>2025-07-28</th>\n",
       "      <td>1355</td>\n",
       "      <td>14905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00bb26ff-6fe3-4465-ac77-12bfc33aa6df</th>\n",
       "      <th>2025-07-17</th>\n",
       "      <td>1787</td>\n",
       "      <td>19657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ef6d2e9-7601-4df6-a215-83e6e79aa24e</th>\n",
       "      <th>2025-10-06</th>\n",
       "      <td>1293</td>\n",
       "      <td>14223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11c49e5d-21ac-4d6d-88c3-f211562a8e07</th>\n",
       "      <th>2025-09-17</th>\n",
       "      <td>1156</td>\n",
       "      <td>12647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1d6dbba7-dcc5-46f4-a4aa-aef3124a8fcf</th>\n",
       "      <th>2025-09-05</th>\n",
       "      <td>1959</td>\n",
       "      <td>21549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2a3f341e-1807-4eb3-9d8d-202c32d52632</th>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>1075</td>\n",
       "      <td>11825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3ecf34fc-1f15-4b32-970f-4061544da763</th>\n",
       "      <th>2025-07-14</th>\n",
       "      <td>1835</td>\n",
       "      <td>20179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43d750b5-8698-4cf0-9ea2-f705f4f196ed</th>\n",
       "      <th>2025-09-25</th>\n",
       "      <td>1968</td>\n",
       "      <td>21585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44d26695-cdf2-41a4-b161-393fdaf964bc</th>\n",
       "      <th>2025-07-26</th>\n",
       "      <td>2122</td>\n",
       "      <td>23342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49c33d7c-ef04-43a7-bbd0-783489c64849</th>\n",
       "      <th>2025-09-06</th>\n",
       "      <td>1757</td>\n",
       "      <td>19327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6a258715-721a-41e9-8abb-af41308c1f48</th>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>2051</td>\n",
       "      <td>22561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6f506df9-be60-452d-b914-8230c29c2ff1</th>\n",
       "      <th>2025-07-22</th>\n",
       "      <td>1622</td>\n",
       "      <td>17838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78a802ae-d6cd-4f39-aecb-138668fa2607</th>\n",
       "      <th>2025-10-02</th>\n",
       "      <td>1139</td>\n",
       "      <td>12529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81ae4870-e57d-4bc4-a2d7-48ffa5411707</th>\n",
       "      <th>2025-07-10</th>\n",
       "      <td>2113</td>\n",
       "      <td>21852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823158da-7b0a-4c19-8189-663c22a3ae38</th>\n",
       "      <th>2025-09-27</th>\n",
       "      <td>2286</td>\n",
       "      <td>25146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8ea67496-0fb3-4efd-8cea-4b8d88351b8e</th>\n",
       "      <th>2025-07-01</th>\n",
       "      <td>1766</td>\n",
       "      <td>11144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91eee220-fee7-488b-952a-c96aa8e493db</th>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>1627</td>\n",
       "      <td>17897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9cd54b2b-31f9-43e4-9073-0d2b61bf9f15</th>\n",
       "      <th>2025-08-30</th>\n",
       "      <td>2039</td>\n",
       "      <td>22429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9defe9fd-0374-4de6-99f7-aaa392903d67</th>\n",
       "      <th>2025-08-12</th>\n",
       "      <td>2244</td>\n",
       "      <td>24684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a1db5d4b-b641-4d3d-a8a6-d4c9bdd9c033</th>\n",
       "      <th>2025-08-23</th>\n",
       "      <td>1543</td>\n",
       "      <td>16973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c5288ca2-3928-4364-8f08-bebc1036dd87</th>\n",
       "      <th>2025-07-11</th>\n",
       "      <td>2626</td>\n",
       "      <td>28801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd4a656f-290a-41e2-be1d-bf62ad85757d</th>\n",
       "      <th>2025-09-29</th>\n",
       "      <td>2186</td>\n",
       "      <td>23998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4b4a349-3b14-439e-946f-f716101dac69</th>\n",
       "      <th>2025-08-02</th>\n",
       "      <td>853</td>\n",
       "      <td>9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e627d7f0-46c4-4894-872e-59a2fc108c30</th>\n",
       "      <th>2025-08-07</th>\n",
       "      <td>356</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e697ab50-0abb-42d3-92a0-43f1ed597476</th>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>3678</td>\n",
       "      <td>31476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f93bf2bd-1d50-4131-9ec2-223a4d9987e8</th>\n",
       "      <th>2025-09-23</th>\n",
       "      <td>2921</td>\n",
       "      <td>31451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CLICK       \n",
       "                                                       sum  count\n",
       "EXPERIMENT_ID                        EXPERIMENT_DATE             \n",
       "002deaf7-331f-4b5e-866b-f6dad60e4a79 2025-07-28       1355  14905\n",
       "00bb26ff-6fe3-4465-ac77-12bfc33aa6df 2025-07-17       1787  19657\n",
       "0ef6d2e9-7601-4df6-a215-83e6e79aa24e 2025-10-06       1293  14223\n",
       "11c49e5d-21ac-4d6d-88c3-f211562a8e07 2025-09-17       1156  12647\n",
       "1d6dbba7-dcc5-46f4-a4aa-aef3124a8fcf 2025-09-05       1959  21549\n",
       "2a3f341e-1807-4eb3-9d8d-202c32d52632 2025-08-25       1075  11825\n",
       "3ecf34fc-1f15-4b32-970f-4061544da763 2025-07-14       1835  20179\n",
       "43d750b5-8698-4cf0-9ea2-f705f4f196ed 2025-09-25       1968  21585\n",
       "44d26695-cdf2-41a4-b161-393fdaf964bc 2025-07-26       2122  23342\n",
       "49c33d7c-ef04-43a7-bbd0-783489c64849 2025-09-06       1757  19327\n",
       "6a258715-721a-41e9-8abb-af41308c1f48 2025-08-19       2051  22561\n",
       "6f506df9-be60-452d-b914-8230c29c2ff1 2025-07-22       1622  17838\n",
       "78a802ae-d6cd-4f39-aecb-138668fa2607 2025-10-02       1139  12529\n",
       "81ae4870-e57d-4bc4-a2d7-48ffa5411707 2025-07-10       2113  21852\n",
       "823158da-7b0a-4c19-8189-663c22a3ae38 2025-09-27       2286  25146\n",
       "8ea67496-0fb3-4efd-8cea-4b8d88351b8e 2025-07-01       1766  11144\n",
       "91eee220-fee7-488b-952a-c96aa8e493db 2025-08-14       1627  17897\n",
       "9cd54b2b-31f9-43e4-9073-0d2b61bf9f15 2025-08-30       2039  22429\n",
       "9defe9fd-0374-4de6-99f7-aaa392903d67 2025-08-12       2244  24684\n",
       "a1db5d4b-b641-4d3d-a8a6-d4c9bdd9c033 2025-08-23       1543  16973\n",
       "c5288ca2-3928-4364-8f08-bebc1036dd87 2025-07-11       2626  28801\n",
       "cd4a656f-290a-41e2-be1d-bf62ad85757d 2025-09-29       2186  23998\n",
       "e4b4a349-3b14-439e-946f-f716101dac69 2025-08-02        853   9356\n",
       "e627d7f0-46c4-4894-872e-59a2fc108c30 2025-08-07        356   3579\n",
       "e697ab50-0abb-42d3-92a0-43f1ed597476 2025-08-29       3678  31476\n",
       "f93bf2bd-1d50-4131-9ec2-223a4d9987e8 2025-09-23       2921  31451"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.groupby([\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]).agg({\"CLICK\": [\"sum\", \"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92a5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_COLS = [\n",
    "    \"RECIPIENT_ID\",\n",
    "    \"COUNTRY\",\n",
    "    \"REGION\",\n",
    "    \"LATEST_CLICK_CLIENT_TYPE\",\n",
    "    \"LATEST_CLICK_CLIENT_NAME\",\n",
    "    \"LATEST_CLICK_CLIENT_OS_FAMILY\",\n",
    "    \"TOTAL_ORDERS_VALUE\",\n",
    "    \"AVG_ORDER_VALUE\",\n",
    "    \"LAST_ORDER_VALUE\",\n",
    "    \"MONTHS_SINCE_FIRST_ACTIVE\",\n",
    "    \"CLICK\",\n",
    "    \"FIRST_UTM_SOURCE\", \"FIRST_UTM_CONTENT\", \"FIRST_UTM_CAMPAIGN\",\n",
    "    'LAST_UTM_SOURCE', 'LAST_UTM_CONTENT', 'LAST_UTM_CAMPAIGN',\n",
    "    \"CITY\",\n",
    "    \"TIMEZONE\"\n",
    "]\n",
    "VARIATION_COLS = [\n",
    "   'Q1_CREATIVE', 'Q2_CREATIVE', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q1_SBL', 'Q2_SBL' \n",
    "]\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    \"COUNTRY\",\n",
    "    \"REGION\",\n",
    "    \"CITY\",\n",
    "    \"TIMEZONE\",\n",
    "    \"LATEST_CLICK_CLIENT_TYPE\",\n",
    "    \"LATEST_CLICK_CLIENT_NAME\",\n",
    "    \"LATEST_CLICK_CLIENT_OS_FAMILY\",\n",
    "    \"FIRST_UTM_SOURCE\", \"FIRST_UTM_CONTENT\", \"FIRST_UTM_CAMPAIGN\",\n",
    "    'LAST_UTM_SOURCE', 'LAST_UTM_CONTENT', 'LAST_UTM_CAMPAIGN',\n",
    "    'Q1_CREATIVE', 'Q2_CREATIVE', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q1_SBL', 'Q2_SBL' \n",
    "]\n",
    "\n",
    "NUMERICAL_COLS = [\n",
    "    \"TOTAL_ORDERS_VALUE\",\n",
    "    \"AVG_ORDER_VALUE\",\n",
    "    \"LAST_ORDER_VALUE\",\n",
    "    \"MONTHS_SINCE_FIRST_ACTIVE\"\n",
    "]\n",
    "COLS = CATEGORICAL_COLS + NUMERICAL_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93dbb1",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ed27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXPERIMENT_ID</th>\n",
       "      <th>VARIATION_ID</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>LAST_CLICKED_VARIATION_ID</th>\n",
       "      <th>RECIPIENT_ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>REGION</th>\n",
       "      <th>...</th>\n",
       "      <th>FIRST_UTM_CONTENT</th>\n",
       "      <th>FIRST_UTM_CAMPAIGN</th>\n",
       "      <th>LAST_UTM_SOURCE</th>\n",
       "      <th>LAST_UTM_CONTENT</th>\n",
       "      <th>LAST_UTM_CAMPAIGN</th>\n",
       "      <th>CLICK_COUNT</th>\n",
       "      <th>CLICK</th>\n",
       "      <th>EXPERIMENT_DATE</th>\n",
       "      <th>FIRST_ACTIVE_TS_dt</th>\n",
       "      <th>MONTHS_SINCE_FIRST_ACTIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e697ab50-0abb-42d3-92a0-43f1ed597476</td>\n",
       "      <td>0cd88d89-da2d-4ff2-a223-189af1cdf1b8</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01JJNJR6V68XNN1WTS6710V6GZ</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>724059624219</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>2025-01-28 04:28:50</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44d26695-cdf2-41a4-b161-393fdaf964bc</td>\n",
       "      <td>2968d945-ec27-4bbb-8a6b-4a92db7266de</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01JWDZHGKRGH94A8X8BRCCWV58</td>\n",
       "      <td>Uzès</td>\n",
       "      <td>France</td>\n",
       "      <td>Gard</td>\n",
       "      <td>...</td>\n",
       "      <td>120223221813940318</td>\n",
       "      <td>tRoas 454 ASC 1.5 Campaign - Full Coverage Com...</td>\n",
       "      <td>Klaviyo</td>\n",
       "      <td>UNK</td>\n",
       "      <td>em - new just dropped 445 lace bra Lilas - Thu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>2025-05-29 12:16:17</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e697ab50-0abb-42d3-92a0-43f1ed597476</td>\n",
       "      <td>71995c0d-3a29-4098-aec0-505734948e83</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01JVZ51X6G5MBB5WCNJ05VBZ7D</td>\n",
       "      <td>Marana</td>\n",
       "      <td>United States</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>...</td>\n",
       "      <td>120207224081300318</td>\n",
       "      <td>Bid Caps Bra</td>\n",
       "      <td>Klaviyo</td>\n",
       "      <td>UNK</td>\n",
       "      <td>em - End of summer sale 457 27.29 - Wed 17 Sep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>2025-04-29 20:38:19</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91eee220-fee7-488b-952a-c96aa8e493db</td>\n",
       "      <td>5238222f-cb69-4156-871e-321b669fe1e5</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01K1Y919AGBK33GCT4TFTVZJQP</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>...</td>\n",
       "      <td>The Comfort Shaping Bra</td>\n",
       "      <td>true</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>2025-08-05 23:28:08</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00bb26ff-6fe3-4465-ac77-12bfc33aa6df</td>\n",
       "      <td>2ebafe74-b5a8-4ff9-890e-a76f96db1741</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01JW87NBGVQHE7XD4YH2NNQR1E</td>\n",
       "      <td>Wesley Chapel</td>\n",
       "      <td>United States</td>\n",
       "      <td>Florida</td>\n",
       "      <td>...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>2025-05-27 06:56:58</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          EXPERIMENT_ID                          VARIATION_ID  \\\n",
       "0  e697ab50-0abb-42d3-92a0-43f1ed597476  0cd88d89-da2d-4ff2-a223-189af1cdf1b8   \n",
       "2  44d26695-cdf2-41a4-b161-393fdaf964bc  2968d945-ec27-4bbb-8a6b-4a92db7266de   \n",
       "3  e697ab50-0abb-42d3-92a0-43f1ed597476  71995c0d-3a29-4098-aec0-505734948e83   \n",
       "4  91eee220-fee7-488b-952a-c96aa8e493db  5238222f-cb69-4156-871e-321b669fe1e5   \n",
       "5  00bb26ff-6fe3-4465-ac77-12bfc33aa6df  2ebafe74-b5a8-4ff9-890e-a76f96db1741   \n",
       "\n",
       "   MONTH  DAY  HOUR LAST_CLICKED_VARIATION_ID                RECIPIENT_ID  \\\n",
       "0      8   29    14                       NaN  01JJNJR6V68XNN1WTS6710V6GZ   \n",
       "2      7   26    13                       NaN  01JWDZHGKRGH94A8X8BRCCWV58   \n",
       "3      8   29    12                       NaN  01JVZ51X6G5MBB5WCNJ05VBZ7D   \n",
       "4      8   14    12                       NaN  01K1Y919AGBK33GCT4TFTVZJQP   \n",
       "5      7   17    13                       NaN  01JW87NBGVQHE7XD4YH2NNQR1E   \n",
       "\n",
       "            CITY        COUNTRY    REGION  ...        FIRST_UTM_CONTENT  \\\n",
       "0       Auckland    New Zealand  Auckland  ...                      UNK   \n",
       "2           Uzès         France      Gard  ...       120223221813940318   \n",
       "3         Marana  United States   Arizona  ...       120207224081300318   \n",
       "4      Melbourne      Australia  Victoria  ...  The Comfort Shaping Bra   \n",
       "5  Wesley Chapel  United States   Florida  ...                      UNK   \n",
       "\n",
       "                                  FIRST_UTM_CAMPAIGN  LAST_UTM_SOURCE  \\\n",
       "0                                       724059624219              UNK   \n",
       "2  tRoas 454 ASC 1.5 Campaign - Full Coverage Com...          Klaviyo   \n",
       "3                                       Bid Caps Bra          Klaviyo   \n",
       "4                                               true              UNK   \n",
       "5                                                UNK              UNK   \n",
       "\n",
       "   LAST_UTM_CONTENT                                  LAST_UTM_CAMPAIGN  \\\n",
       "0               UNK                                                UNK   \n",
       "2               UNK  em - new just dropped 445 lace bra Lilas - Thu...   \n",
       "3               UNK  em - End of summer sale 457 27.29 - Wed 17 Sep...   \n",
       "4               UNK                                                UNK   \n",
       "5               UNK                                                UNK   \n",
       "\n",
       "   CLICK_COUNT CLICK EXPERIMENT_DATE  FIRST_ACTIVE_TS_dt  \\\n",
       "0            2     1      2025-08-29 2025-01-28 04:28:50   \n",
       "2            1     1      2025-07-26 2025-05-29 12:16:17   \n",
       "3            1     1      2025-08-29 2025-04-29 20:38:19   \n",
       "4            1     1      2025-08-14 2025-08-05 23:28:08   \n",
       "5            1     1      2025-07-17 2025-05-27 06:56:58   \n",
       "\n",
       "  MONTHS_SINCE_FIRST_ACTIVE  \n",
       "0                       9.0  \n",
       "2                       5.0  \n",
       "3                       6.0  \n",
       "4                       2.0  \n",
       "5                       5.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics import hit_rate_at_k, mrr_at_k\n",
    "from models import get_model, get_pooled_dataset\n",
    "\n",
    "users_df.head()\n",
    "train_data = get_pooled_dataset(users_df, pos_neg_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a6767",
   "metadata": {},
   "source": [
    "### Ranking Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0742bf",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89c5723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search 1 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 2 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 3 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 4 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 5 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 6 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 7 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 8 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 9 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 10 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 11 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 12 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 13 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 14 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 15 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 16 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 17 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 18 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 19 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 20 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 21 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 22 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 23 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 24 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 25 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 26 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 27 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"RankerGridSearch\")\n",
    "\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.03, 0.1, 0.5],\n",
    "    \"depth\": [3, 6, 10],\n",
    "    \"iterations\": [200, 500, 1000],\n",
    "    # \"l2_leaf_reg\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "for i, params in enumerate(grid):\n",
    "    print(f\"Running grid search {i+1} of {params}\")\n",
    "    with mlflow.start_run(run_name=f\"ranker_grid_search_{i}\"):\n",
    "        # Ensure experiment_date is datetime\n",
    "        users_df[\"EXPERIMENT_DATE\"] = pd.to_datetime(users_df[\"EXPERIMENT_DATE\"])\n",
    "\n",
    "        variations_per_experimen_df = users_df[\n",
    "            [\"EXPERIMENT_ID\", \"VARIATION_ID\"]\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        # Sort unique experiments by date\n",
    "        experiment_order = (\n",
    "            users_df[[\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]]\n",
    "            .sort_values(\"EXPERIMENT_DATE\")\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        n_last_test = 4\n",
    "        n_last_val = 2\n",
    "        n_last_train = n_last_test + n_last_val\n",
    "\n",
    "        # Get last two for test, others for train\n",
    "        test_experiments = experiment_order.tail(n_last_test)[\"EXPERIMENT_ID\"]\n",
    "        val_experiments = experiment_order.iloc[-n_last_train:-n_last_test][\n",
    "            \"EXPERIMENT_ID\"\n",
    "        ]\n",
    "        train_experiments = experiment_order.iloc[:-n_last_train][\"EXPERIMENT_ID\"]\n",
    "\n",
    "        print(f\"Number of train experiments: {len(train_experiments)}\")\n",
    "        print(f\"Number of validation experiments: {len(val_experiments)}\")\n",
    "        print(f\"Number of test experiments: {len(test_experiments)}\")\n",
    "        assert len(train_experiments) + len(val_experiments) + len(\n",
    "            test_experiments\n",
    "        ) == len(experiment_order)\n",
    "        # Join users_df with variation_df on EXPERIMENT_ID and VARIATION_ID\n",
    "\n",
    "        users_all_variations = pd.merge(\n",
    "            users_df.drop(columns=[\"VARIATION_ID\"]),\n",
    "            variations_per_experimen_df,\n",
    "            how=\"left\",\n",
    "            left_on=\"EXPERIMENT_ID\",\n",
    "            right_on=\"EXPERIMENT_ID\",\n",
    "        )\n",
    "        # Assign the click to the correct variation\n",
    "        users_all_variations[\"CLICK\"] = (\n",
    "            users_all_variations.set_index(\n",
    "                [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "            )\n",
    "            .index.map(\n",
    "                users_df.drop_duplicates(\n",
    "                    [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "                ).set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])[\"CLICK\"]\n",
    "            )\n",
    "            .fillna(0.5)\n",
    "        )\n",
    "\n",
    "        users_all_variations = users_all_variations.merge(\n",
    "            variations_df,\n",
    "            left_on=[\"VARIATION_ID\"],\n",
    "            right_on=[\"VARIATION_ID\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Select rows for train/test\n",
    "        train_df = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(train_experiments)\n",
    "        ]\n",
    "        # For validation set\n",
    "        val_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(val_experiments)\n",
    "        ]\n",
    "        val_df = val_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        # For test set\n",
    "        test_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(test_experiments)\n",
    "        ]\n",
    "        test_df = test_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        train_pool, _, X_train, y_train = get_pooled_dataset(train_df, pos_neg_ratio=1)\n",
    "        val_pool, _, X_val, y_val = get_pooled_dataset(val_df)\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        ranker = get_model(\"ranker\", cat_features, params)\n",
    "        ranker.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "        test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(val_df)\n",
    "        scores = ranker.predict(X_test)\n",
    "\n",
    "        preds = val_df.sort_values([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).assign(\n",
    "            PRED=scores, GT=y_test\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = val_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        mrr_at_5, uplift_mrr_at_5 = mrr_at_k(preds, y_true, 5)\n",
    "        hit_rate_at_1, uplift_hit_rate_at_1 = hit_rate_at_k(preds, y_true, 1)\n",
    "\n",
    "        mlflow.log_metric(\"mrr_at_5\", mrr_at_5)\n",
    "        mlflow.log_metric(\"mrr_at_5_uplift\", uplift_mrr_at_5)\n",
    "        mlflow.log_metric(\"hit_rate_at_1\", hit_rate_at_1)\n",
    "        mlflow.log_metric(\"hit_rate_at_1_uplift\", uplift_hit_rate_at_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335d7d6",
   "metadata": {},
   "source": [
    "**PN Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c829a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/22 19:57:40 INFO mlflow.tracking.fluent: Experiment with name 'PN Ratio' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search 1 of pn_ratio: 0\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  176967\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 2 of pn_ratio: 1\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  354542\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 3 of pn_ratio: 2\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  532277\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 4 of pn_ratio: 3\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  709938\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 5 of pn_ratio: 4\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  887454\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 6 of pn_ratio: 5\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  1065103\n",
      "The size of the dataset is:  24445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 557, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is:  24445\n",
      "Running grid search 7 of pn_ratio: 6\n",
      "Number of train experiments: 21\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  1242822\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 8 of pn_ratio: 7\n",
      "Number of train experiments: 21\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  1420560\n",
      "The size of the dataset is:  24445\n",
      "The size of the dataset is:  24445\n",
      "Running grid search 9 of pn_ratio: 8\n",
      "Number of train experiments: 21\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  1597962\n",
      "The size of the dataset is:  24445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 624, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is:  24445\n",
      "Running grid search 10 of pn_ratio: 9\n",
      "Number of train experiments: 21\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "The size of the dataset is:  1775746\n",
      "The size of the dataset is:  24445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 576, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is:  24445\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"PN Ratio\")\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "model_params = {\n",
    "    \"learning_rate\": 0.5,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 1000,\n",
    "    # \"l2_leaf_reg\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "pn_grid = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "for i, pn_ratio in enumerate(pn_grid):\n",
    "    print(f\"Running grid search {i+1} of pn_ratio: {pn_ratio}\")\n",
    "    with mlflow.start_run(run_name=f\"ranker_pn_search_{i}\"):\n",
    "        # Ensure experiment_date is datetime\n",
    "        users_df[\"EXPERIMENT_DATE\"] = pd.to_datetime(users_df[\"EXPERIMENT_DATE\"])\n",
    "\n",
    "        variations_per_experimen_df = users_df[\n",
    "            [\"EXPERIMENT_ID\", \"VARIATION_ID\"]\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        # Sort unique experiments by date\n",
    "        experiment_order = (\n",
    "            users_df[[\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]]\n",
    "            .sort_values(\"EXPERIMENT_DATE\")\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        n_last_test = 4\n",
    "        n_last_val = 2\n",
    "        n_last_train = n_last_test + n_last_val\n",
    "\n",
    "        # Get last two for test, others for train\n",
    "        test_experiments = experiment_order.tail(n_last_test)[\"EXPERIMENT_ID\"]\n",
    "        val_experiments = experiment_order.iloc[-n_last_train:-n_last_test][\n",
    "            \"EXPERIMENT_ID\"\n",
    "        ]\n",
    "        train_experiments = experiment_order.iloc[:-n_last_train][\"EXPERIMENT_ID\"]\n",
    "\n",
    "        print(f\"Number of train experiments: {len(train_experiments)}\")\n",
    "        print(f\"Number of validation experiments: {len(val_experiments)}\")\n",
    "        print(f\"Number of test experiments: {len(test_experiments)}\")\n",
    "        assert len(train_experiments) + len(val_experiments) + len(\n",
    "            test_experiments\n",
    "        ) == len(experiment_order)\n",
    "        # Join users_df with variation_df on EXPERIMENT_ID and VARIATION_ID\n",
    "\n",
    "        users_all_variations = pd.merge(\n",
    "            users_df.drop(columns=[\"VARIATION_ID\"]),\n",
    "            variations_per_experimen_df,\n",
    "            how=\"left\",\n",
    "            left_on=\"EXPERIMENT_ID\",\n",
    "            right_on=\"EXPERIMENT_ID\",\n",
    "        )\n",
    "        # Assign the click to the correct variation\n",
    "        users_all_variations[\"CLICK\"] = (\n",
    "            users_all_variations.set_index(\n",
    "                [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "            )\n",
    "            .index.map(\n",
    "                users_df.drop_duplicates(\n",
    "                    [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "                ).set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])[\"CLICK\"]\n",
    "            )\n",
    "            .fillna(0.5)\n",
    "        )\n",
    "\n",
    "        users_all_variations = users_all_variations.merge(\n",
    "            variations_df,\n",
    "            left_on=[\"VARIATION_ID\"],\n",
    "            right_on=[\"VARIATION_ID\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Select rows for train/test\n",
    "        train_df = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(train_experiments)\n",
    "        ]\n",
    "        # For validation set\n",
    "        val_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(val_experiments)\n",
    "        ]\n",
    "        val_df = val_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        # For test set\n",
    "        test_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(test_experiments)\n",
    "        ]\n",
    "        test_df = test_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        train_pool, _, X_train, y_train = get_pooled_dataset(train_df, pos_neg_ratio=pn_ratio)\n",
    "        val_pool, _, X_val, y_val = get_pooled_dataset(val_df)\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        ranker = get_model(\"ranker\", cat_features, model_params)\n",
    "        ranker.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "        test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(val_df)\n",
    "        scores = ranker.predict(X_test)\n",
    "\n",
    "        preds = val_df.sort_values([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).assign(\n",
    "            PRED=scores, GT=y_test\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = val_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        mrr_at_5, uplift_mrr_at_5 = mrr_at_k(preds, y_true, 5)\n",
    "        hit_rate_at_1, uplift_hit_rate_at_1 = hit_rate_at_k(preds, y_true, 1)\n",
    "\n",
    "        mlflow.log_metric(\"mrr_at_5\", mrr_at_5)\n",
    "        mlflow.log_metric(\"mrr_at_5_uplift\", uplift_mrr_at_5)\n",
    "        mlflow.log_metric(\"hit_rate_at_1\", hit_rate_at_1)\n",
    "        mlflow.log_metric(\"hit_rate_at_1_uplift\", uplift_hit_rate_at_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d2ef2",
   "metadata": {},
   "source": [
    "### Train catboost via expanding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4b6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_per_experimen_df = users_df[\n",
    "    [\"EXPERIMENT_ID\", \"VARIATION_ID\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "users_all_variations = pd.merge(\n",
    "    users_df.drop(columns=[\"VARIATION_ID\"]),\n",
    "    variations_per_experimen_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"EXPERIMENT_ID\",\n",
    "    right_on=\"EXPERIMENT_ID\",\n",
    ")\n",
    "# Assign the click to the correct variation\n",
    "users_all_variations[\"CLICK\"] = (\n",
    "    users_all_variations.set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])\n",
    "    .index.map(\n",
    "        users_df.drop_duplicates(\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "        ).set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])[\"CLICK\"]\n",
    "    )\n",
    "    .fillna(0.5)\n",
    ")\n",
    "\n",
    "users_all_variations = users_all_variations.merge(\n",
    "    variations_df,\n",
    "    left_on=[\"VARIATION_ID\"],\n",
    "    right_on=[\"VARIATION_ID\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c537fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train idx: [0 1 2 3 4 5], Test idx: [6 7 8 9]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.1975163715843296 uplift_hit_rate: -6.663854067155237 mrr: 0.4609974182112353 uplift_mrr: -2.468728386497536\n",
      "Train idx: [0 1 2 3 4 5 6 7 8 9], Test idx: [10 11 12 13]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.18878523085612545 uplift_hit_rate: -5.607384571937282 mrr: 0.4500056619439219 uplift_mrr: -1.4586141728638173\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Test idx: [14 15 16 17]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.2081134850303727 uplift_hit_rate: 4.0567425151863405 mrr: 0.46701370542355036 uplift_mrr: 2.265774910266509\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17], Test idx: [18 19 20 21]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.17698985806504441 uplift_hit_rate: -11.505070967477801 mrr: 0.4419719346477357 uplift_mrr: -3.2178245296929013\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21], Test idx: [22 23 24 25]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.2332073676466253 uplift_hit_rate: 2.185719733566208 mrr: 0.496386243737362 uplift_mrr: 1.3894457910508333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Start an mlflow experiment\n",
    "mlflow.set_experiment(\"Catboost_Ranker\")\n",
    "\n",
    "pos_neg_ratio = 1\n",
    "experiment_name = f\"pn_ratio_{pos_neg_ratio}_all_feats\"\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{experiment_name}\"):\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=4)\n",
    "    idx = (\n",
    "        users_df[[\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]]\n",
    "        .sort_values(\"EXPERIMENT_DATE\")\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(idx):\n",
    "        print(f\"Train idx: {train_idx}, Test idx: {test_idx}\")\n",
    "        train_idx = idx.iloc[train_idx][\"EXPERIMENT_ID\"].values\n",
    "        test_idx = idx.iloc[test_idx][\"EXPERIMENT_ID\"].values\n",
    "\n",
    "        # prepare train data\n",
    "        train_df = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(train_idx)\n",
    "        ]\n",
    "\n",
    "        train_pool, _, X_train, y_train = get_pooled_dataset(train_df, pos_neg_ratio)\n",
    "\n",
    "        # prepare test data\n",
    "        test_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(test_idx)\n",
    "        ]\n",
    "        test_df = test_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(test_df)\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        # Train CatBoost model\n",
    "\n",
    "        print(\"Training CatBoost model...\")\n",
    "        model = get_model(\"ranker\", cat_features)\n",
    "        model.fit(train_pool)\n",
    "\n",
    "        scores = model.predict(X_test)\n",
    "        preds = test_df.sort_values([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).assign(\n",
    "            PRED=scores, GT=y_test\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = test_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        hit_rate, uplift_hit_rate = hit_rate_at_k(preds, y_true, k=1)\n",
    "        mrr, uplift_mrr = mrr_at_k(preds, y_true, 5)\n",
    "\n",
    "        # Gather the metrics for this split. You could add more metrics if needed.\n",
    "        mlflow.log_metric(\"avg_hit_rate_at_1\", hit_rate, step=len(train_idx))\n",
    "        mlflow.log_metric(\"avg_mrr_at_5\", mrr, step=len(train_idx))\n",
    "        mlflow.log_metric(\n",
    "            \"avg_uplift_hit_rate_at_1\", uplift_hit_rate, step=len(train_idx)\n",
    "        )\n",
    "        mlflow.log_metric(\"avg_uplift_mrr_at_5\", uplift_mrr, step=len(train_idx))\n",
    "\n",
    "        print(\n",
    "            \"hit_rate:\",\n",
    "            hit_rate,\n",
    "            \"uplift_hit_rate:\",\n",
    "            uplift_hit_rate,\n",
    "            \"mrr:\",\n",
    "            mrr,\n",
    "            \"uplift_mrr:\",\n",
    "            uplift_mrr,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3c346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eikona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
