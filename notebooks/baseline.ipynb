{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d85fa04",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e61557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/broniy/Desktop/CreativeRank/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "from metrics import bootstrap_mrr_at_k, mrr_at_k, hit_rate_at_k\n",
    "from models import get_model, get_pooled_dataset\n",
    "from settings import DATA_FOLDER\n",
    "from notebooks.experiment_data import get_experiment_data, COLS, CATEGORICAL_COLS, split_experiment_train_test_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646cd390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_df size before removing small experiments: 501008 rows\n",
      "users_df size after removing small experiments: 500953 rows\n"
     ]
    }
   ],
   "source": [
    "data = get_experiment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4295346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">CLICK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXPERIMENT_DATE</th>\n",
       "      <th>EXPERIMENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-01</th>\n",
       "      <th>8ea67496-0fb3-4efd-8cea-4b8d88351b8e</th>\n",
       "      <td>1766.0</td>\n",
       "      <td>11144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-10</th>\n",
       "      <th>81ae4870-e57d-4bc4-a2d7-48ffa5411707</th>\n",
       "      <td>2113.0</td>\n",
       "      <td>21852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-11</th>\n",
       "      <th>c5288ca2-3928-4364-8f08-bebc1036dd87</th>\n",
       "      <td>2626.0</td>\n",
       "      <td>28807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-14</th>\n",
       "      <th>3ecf34fc-1f15-4b32-970f-4061544da763</th>\n",
       "      <td>1835.0</td>\n",
       "      <td>20187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17</th>\n",
       "      <th>00bb26ff-6fe3-4465-ac77-12bfc33aa6df</th>\n",
       "      <td>1787.0</td>\n",
       "      <td>19663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-22</th>\n",
       "      <th>6f506df9-be60-452d-b914-8230c29c2ff1</th>\n",
       "      <td>1622.0</td>\n",
       "      <td>17838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-26</th>\n",
       "      <th>44d26695-cdf2-41a4-b161-393fdaf964bc</th>\n",
       "      <td>2122.0</td>\n",
       "      <td>23348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-28</th>\n",
       "      <th>002deaf7-331f-4b5e-866b-f6dad60e4a79</th>\n",
       "      <td>1355.0</td>\n",
       "      <td>14905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-02</th>\n",
       "      <th>e4b4a349-3b14-439e-946f-f716101dac69</th>\n",
       "      <td>853.0</td>\n",
       "      <td>9410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-07</th>\n",
       "      <th>e627d7f0-46c4-4894-872e-59a2fc108c30</th>\n",
       "      <td>356.0</td>\n",
       "      <td>3951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-12</th>\n",
       "      <th>9defe9fd-0374-4de6-99f7-aaa392903d67</th>\n",
       "      <td>2244.0</td>\n",
       "      <td>24684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <th>91eee220-fee7-488b-952a-c96aa8e493db</th>\n",
       "      <td>1627.0</td>\n",
       "      <td>17897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <th>6a258715-721a-41e9-8abb-af41308c1f48</th>\n",
       "      <td>2051.0</td>\n",
       "      <td>22561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-23</th>\n",
       "      <th>a1db5d4b-b641-4d3d-a8a6-d4c9bdd9c033</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>16973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <th>2a3f341e-1807-4eb3-9d8d-202c32d52632</th>\n",
       "      <td>1075.0</td>\n",
       "      <td>11825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <th>e697ab50-0abb-42d3-92a0-43f1ed597476</th>\n",
       "      <td>3678.0</td>\n",
       "      <td>31476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-30</th>\n",
       "      <th>9cd54b2b-31f9-43e4-9073-0d2b61bf9f15</th>\n",
       "      <td>2039.0</td>\n",
       "      <td>22429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-05</th>\n",
       "      <th>1d6dbba7-dcc5-46f4-a4aa-aef3124a8fcf</th>\n",
       "      <td>1959.0</td>\n",
       "      <td>21549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-06</th>\n",
       "      <th>49c33d7c-ef04-43a7-bbd0-783489c64849</th>\n",
       "      <td>1757.0</td>\n",
       "      <td>19327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-17</th>\n",
       "      <th>11c49e5d-21ac-4d6d-88c3-f211562a8e07</th>\n",
       "      <td>1156.0</td>\n",
       "      <td>12853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-23</th>\n",
       "      <th>f93bf2bd-1d50-4131-9ec2-223a4d9987e8</th>\n",
       "      <td>2921.0</td>\n",
       "      <td>32065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-25</th>\n",
       "      <th>43d750b5-8698-4cf0-9ea2-f705f4f196ed</th>\n",
       "      <td>1968.0</td>\n",
       "      <td>21909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-27</th>\n",
       "      <th>823158da-7b0a-4c19-8189-663c22a3ae38</th>\n",
       "      <td>2286.0</td>\n",
       "      <td>25146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29</th>\n",
       "      <th>cd4a656f-290a-41e2-be1d-bf62ad85757d</th>\n",
       "      <td>2186.0</td>\n",
       "      <td>24260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-02</th>\n",
       "      <th>78a802ae-d6cd-4f39-aecb-138668fa2607</th>\n",
       "      <td>1139.0</td>\n",
       "      <td>12529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-06</th>\n",
       "      <th>0ef6d2e9-7601-4df6-a215-83e6e79aa24e</th>\n",
       "      <td>1293.0</td>\n",
       "      <td>14223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       CLICK       \n",
       "                                                         sum  count\n",
       "EXPERIMENT_DATE EXPERIMENT_ID                                      \n",
       "2025-07-01      8ea67496-0fb3-4efd-8cea-4b8d88351b8e  1766.0  11144\n",
       "2025-07-10      81ae4870-e57d-4bc4-a2d7-48ffa5411707  2113.0  21852\n",
       "2025-07-11      c5288ca2-3928-4364-8f08-bebc1036dd87  2626.0  28807\n",
       "2025-07-14      3ecf34fc-1f15-4b32-970f-4061544da763  1835.0  20187\n",
       "2025-07-17      00bb26ff-6fe3-4465-ac77-12bfc33aa6df  1787.0  19663\n",
       "2025-07-22      6f506df9-be60-452d-b914-8230c29c2ff1  1622.0  17838\n",
       "2025-07-26      44d26695-cdf2-41a4-b161-393fdaf964bc  2122.0  23348\n",
       "2025-07-28      002deaf7-331f-4b5e-866b-f6dad60e4a79  1355.0  14905\n",
       "2025-08-02      e4b4a349-3b14-439e-946f-f716101dac69   853.0   9410\n",
       "2025-08-07      e627d7f0-46c4-4894-872e-59a2fc108c30   356.0   3951\n",
       "2025-08-12      9defe9fd-0374-4de6-99f7-aaa392903d67  2244.0  24684\n",
       "2025-08-14      91eee220-fee7-488b-952a-c96aa8e493db  1627.0  17897\n",
       "2025-08-19      6a258715-721a-41e9-8abb-af41308c1f48  2051.0  22561\n",
       "2025-08-23      a1db5d4b-b641-4d3d-a8a6-d4c9bdd9c033  1543.0  16973\n",
       "2025-08-25      2a3f341e-1807-4eb3-9d8d-202c32d52632  1075.0  11825\n",
       "2025-08-29      e697ab50-0abb-42d3-92a0-43f1ed597476  3678.0  31476\n",
       "2025-08-30      9cd54b2b-31f9-43e4-9073-0d2b61bf9f15  2039.0  22429\n",
       "2025-09-05      1d6dbba7-dcc5-46f4-a4aa-aef3124a8fcf  1959.0  21549\n",
       "2025-09-06      49c33d7c-ef04-43a7-bbd0-783489c64849  1757.0  19327\n",
       "2025-09-17      11c49e5d-21ac-4d6d-88c3-f211562a8e07  1156.0  12853\n",
       "2025-09-23      f93bf2bd-1d50-4131-9ec2-223a4d9987e8  2921.0  32065\n",
       "2025-09-25      43d750b5-8698-4cf0-9ea2-f705f4f196ed  1968.0  21909\n",
       "2025-09-27      823158da-7b0a-4c19-8189-663c22a3ae38  2286.0  25146\n",
       "2025-09-29      cd4a656f-290a-41e2-be1d-bf62ad85757d  2186.0  24260\n",
       "2025-10-02      78a802ae-d6cd-4f39-aecb-138668fa2607  1139.0  12529\n",
       "2025-10-06      0ef6d2e9-7601-4df6-a215-83e6e79aa24e  1293.0  14223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values([\"EXPERIMENT_DATE\"]).query(\"CLICK.isin([0, 1])\").groupby(\n",
    "    [\"EXPERIMENT_DATE\", \"EXPERIMENT_ID\"]\n",
    ").agg({\"CLICK\": [\"sum\", \"count\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93dbb1",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4ed27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import hit_rate_at_k, mrr_at_k, bootstrap_mrr_at_k\n",
    "from models import get_model, get_pooled_dataset\n",
    "\n",
    "users_df.head()\n",
    "\n",
    "train_data = users_all_variations[users_all_variations[\"EXPERIMENT_ID\"]=='823158da-7b0a-4c19-8189-663c22a3ae38']\n",
    "test_data = users_all_variations[users_all_variations[\"EXPERIMENT_ID\"]=='1d6dbba7-dcc5-46f4-a4aa-aef3124a8fcf']\n",
    "\n",
    "train_df, train_pool, group_ids, X_train, y_train = get_pooled_dataset(train_data, pos_neg_ratio=1, cols=COLS, cat_cols=CATEGORICAL_COLS)\n",
    "test_df, test_pool, group_ids, X_test, y_test = get_pooled_dataset(test_data, pos_neg_ratio=1, cols=COLS, cat_cols=CATEGORICAL_COLS)\n",
    "\n",
    "ranker = get_model(\"ranker\", train_pool.get_cat_feature_indices(), params={\"depth\": 6, \"learning_rate\": 0.5, \"iterations\": 1000})\n",
    "ranker.fit(train_pool)\n",
    "\n",
    "scores = ranker.predict(test_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04081e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47392530202484273 0.007637761761350472\n"
     ]
    }
   ],
   "source": [
    "preds = test_df.assign(\n",
    "    PRED=scores, GT=y_test\n",
    ")[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "\n",
    "y_true = test_df[\n",
    "    [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "].query(\"CLICK==1\")\n",
    "\n",
    "bootstraped_mrr_at_5, std_mrr_at_5 = bootstrap_mrr_at_k(preds, y_true, 5)\n",
    "print(bootstraped_mrr_at_5, std_mrr_at_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a6767",
   "metadata": {},
   "source": [
    "### Ranking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Baseline\")\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "model_params = dict(\n",
    "    iterations=200,\n",
    "    learning_rate=0.03,\n",
    "    depth=4,\n",
    "    l2_leaf_reg=10,\n",
    "    random_strength=2,\n",
    "    bagging_temperature=0.5,\n",
    "    subsample=0.7,\n",
    "    rsm=0.7,  # feature subsampling\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"ranking\"):\n",
    "\n",
    "    train_data, _, test_data = split_experiment_train_test_val_data(data, n_last_test=4, n_last_val=0)\n",
    "\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "\n",
    "    n_splits = 5\n",
    "    # Use \"EXPERIMENT_ID\" to group\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    groups = train_data[\"EXPERIMENT_ID\"]\n",
    "\n",
    "    cv_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(group_kfold.split(train_data, groups=groups)):\n",
    "        fold_train_data = train_data.iloc[train_idx]\n",
    "        fold_val_data = train_data.iloc[val_idx]\n",
    "\n",
    "        # Prepare pools and datasets per fold\n",
    "        train_df, train_pool, _, X_train, y_train = get_pooled_dataset(\n",
    "            fold_train_data, pos_neg_ratio=1, cols=COLS, cat_cols=CATEGORICAL_COLS\n",
    "        )\n",
    "        val_df, val_pool, _, X_val, y_val = get_pooled_dataset(\n",
    "            fold_val_data, cols=COLS, cat_cols=CATEGORICAL_COLS\n",
    "        )\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        # Fit the model\n",
    "        ranker = get_model(\"ranker\", cat_features, model_params)\n",
    "        ranker.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "        # Validation scoring\n",
    "        scores = ranker.predict(X_val)\n",
    "        preds = val_df.assign(\n",
    "            PRED=scores\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = val_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        avg_mrr_at_5, _, _ = mrr_at_k(preds, y_true, 5, prefix=f\"cvfold{fold}_\")\n",
    "        avg_hit_rate_1, _ = hit_rate_at_k(preds, y_true, 1, prefix=f\"cvfold{fold}_\")\n",
    "\n",
    "        cv_results.append(\n",
    "            {\n",
    "                \"fold\": fold,\n",
    "                \"avg_mrr_at_5\": avg_mrr_at_5,\n",
    "                \"avg_hit_rate_1\": avg_hit_rate_1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"CV results:\", cv_results)\n",
    "\n",
    "    train_df, train_pool, _, X_train, y_train = get_pooled_dataset(train_data, pos_neg_ratio=1, cols=COLS, cat_cols=CATEGORICAL_COLS)\n",
    "    val_df, val_pool, _, X_val, y_val = get_pooled_dataset(val_data, cols=COLS, cat_cols=CATEGORICAL_COLS)\n",
    "    cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "    ranker = get_model(\"ranker\", cat_features, model_params)\n",
    "    ranker.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "    test_df, test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(test_data, cols=COLS, cat_cols=CATEGORICAL_COLS)\n",
    "\n",
    "    for prefix, df, X in [(\"test_\", test_df, X_test), (\"val_\", val_df, X_val)]:\n",
    "        scores = ranker.predict(X)\n",
    "        preds = df.assign(\n",
    "            PRED=scores\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        avg_mrr_at_5, _, _  = mrr_at_k(preds, y_true, 5, prefix=prefix)\n",
    "        avg_hit_rate_1, _ = hit_rate_at_k(preds, y_true, 1, prefix=prefix)\n",
    "        bootstraped_mrr_at_5, std_mrr_at_5 = bootstrap_mrr_at_k(preds, y_true, 5, prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0742bf",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89c5723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search 1 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 2 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 3 of {'depth': 3, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 4 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 5 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 6 of {'depth': 3, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 7 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 8 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 9 of {'depth': 3, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 10 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 11 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 12 of {'depth': 6, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 13 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 14 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 15 of {'depth': 6, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 16 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 17 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 18 of {'depth': 6, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 19 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 20 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 21 of {'depth': 10, 'iterations': 200, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 22 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 23 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 24 of {'depth': 10, 'iterations': 500, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 25 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 26 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n",
      "Running grid search 27 of {'depth': 10, 'iterations': 1000, 'learning_rate': 0.5}\n",
      "Number of train experiments: 20\n",
      "Number of validation experiments: 2\n",
      "Number of test experiments: 4\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"RankerGridSearch\")\n",
    "\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    'iterations': [200, 400, 600],\n",
    "    'learning_rate': [0.02, 0.03, 0.05],\n",
    "    'depth': [3, 4, 5],\n",
    "    'l2_leaf_reg': [3, 5, 10, 20],\n",
    "    'random_strength': [0.5, 1, 2],\n",
    "    'bagging_temperature': [0.25, 0.5, 1],\n",
    "    'rsm': [0.6, 0.8, 1.0],  # feature subsampling\n",
    "    'loss_function': ['YetiRank', 'PairLogit'],\n",
    "    'bootstrap_type': ['Bayesian', 'Bernoulli'],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "for i, params in enumerate(grid):\n",
    "    print(f\"Running grid search {i+1} of {params}\")\n",
    "    with mlflow.start_run(run_name=f\"ranker_grid_search_{i}\"):\n",
    "        # Ensure experiment_date is datetime\n",
    "        users_df[\"EXPERIMENT_DATE\"] = pd.to_datetime(users_df[\"EXPERIMENT_DATE\"])\n",
    "\n",
    "        variations_per_experimen_df = users_df[\n",
    "            [\"EXPERIMENT_ID\", \"VARIATION_ID\"]\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        # Sort unique experiments by date\n",
    "        experiment_order = (\n",
    "            users_df[[\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]]\n",
    "            .sort_values(\"EXPERIMENT_DATE\")\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        n_last_test = 4\n",
    "        n_last_val = 2\n",
    "        n_last_train = n_last_test + n_last_val\n",
    "\n",
    "        # Get last two for test, others for train\n",
    "        test_experiments = experiment_order.tail(n_last_test)[\"EXPERIMENT_ID\"]\n",
    "        val_experiments = experiment_order.iloc[-n_last_train:-n_last_test][\n",
    "            \"EXPERIMENT_ID\"\n",
    "        ]\n",
    "        train_experiments = experiment_order.iloc[:-n_last_train][\"EXPERIMENT_ID\"]\n",
    "\n",
    "        print(f\"Number of train experiments: {len(train_experiments)}\")\n",
    "        print(f\"Number of validation experiments: {len(val_experiments)}\")\n",
    "        print(f\"Number of test experiments: {len(test_experiments)}\")\n",
    "        assert len(train_experiments) + len(val_experiments) + len(\n",
    "            test_experiments\n",
    "        ) == len(experiment_order)\n",
    "        # Join users_df with variation_df on EXPERIMENT_ID and VARIATION_ID\n",
    "\n",
    "        users_all_variations = pd.merge(\n",
    "            users_df.drop(columns=[\"VARIATION_ID\"]),\n",
    "            variations_per_experimen_df,\n",
    "            how=\"left\",\n",
    "            left_on=\"EXPERIMENT_ID\",\n",
    "            right_on=\"EXPERIMENT_ID\",\n",
    "        )\n",
    "        # Assign the click to the correct variation\n",
    "        users_all_variations[\"CLICK\"] = (\n",
    "            users_all_variations.set_index(\n",
    "                [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "            )\n",
    "            .index.map(\n",
    "                users_df.drop_duplicates(\n",
    "                    [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "                ).set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])[\"CLICK\"]\n",
    "            )\n",
    "            .fillna(0.5)\n",
    "        )\n",
    "\n",
    "        users_all_variations = users_all_variations.merge(\n",
    "            variations_df,\n",
    "            left_on=[\"VARIATION_ID\"],\n",
    "            right_on=[\"VARIATION_ID\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Select rows for train/test\n",
    "        train_df = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(train_experiments)\n",
    "        ]\n",
    "        # For validation set\n",
    "        val_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(val_experiments)\n",
    "        ]\n",
    "        val_df = val_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        # For test set\n",
    "        test_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(test_experiments)\n",
    "        ]\n",
    "        test_df = test_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        train_pool, _, X_train, y_train = get_pooled_dataset(train_df, pos_neg_ratio=1)\n",
    "        val_pool, _, X_val, y_val = get_pooled_dataset(val_df)\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        ranker = get_model(\"ranker\", cat_features, params)\n",
    "        ranker.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "        test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(val_df)\n",
    "        scores = ranker.predict(X_test)\n",
    "\n",
    "        preds = val_df.sort_values([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).assign(\n",
    "            PRED=scores, GT=y_test\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = val_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        mrr_at_5, uplift_mrr_at_5 = mrr_at_k(preds, y_true, 5)\n",
    "        hit_rate_at_1, uplift_hit_rate_at_1 = hit_rate_at_k(preds, y_true, 1)\n",
    "\n",
    "        mlflow.log_metric(\"mrr_at_5\", mrr_at_5)\n",
    "        mlflow.log_metric(\"mrr_at_5_uplift\", uplift_mrr_at_5)\n",
    "        mlflow.log_metric(\"hit_rate_at_1\", hit_rate_at_1)\n",
    "        mlflow.log_metric(\"hit_rate_at_1_uplift\", uplift_hit_rate_at_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d2ef2",
   "metadata": {},
   "source": [
    "### Train catboost via expanding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4b6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_per_experimen_df = users_df[\n",
    "    [\"EXPERIMENT_ID\", \"VARIATION_ID\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "users_all_variations = pd.merge(\n",
    "    users_df.drop(columns=[\"VARIATION_ID\"]),\n",
    "    variations_per_experimen_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"EXPERIMENT_ID\",\n",
    "    right_on=\"EXPERIMENT_ID\",\n",
    ")\n",
    "# Assign the click to the correct variation\n",
    "users_all_variations[\"CLICK\"] = (\n",
    "    users_all_variations.set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])\n",
    "    .index.map(\n",
    "        users_df.drop_duplicates(\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"]\n",
    "        ).set_index([\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\"])[\"CLICK\"]\n",
    "    )\n",
    "    .fillna(0.5)\n",
    ")\n",
    "\n",
    "users_all_variations = users_all_variations.merge(\n",
    "    variations_df,\n",
    "    left_on=[\"VARIATION_ID\"],\n",
    "    right_on=[\"VARIATION_ID\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c537fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train idx: [0 1 2 3 4 5], Test idx: [6 7 8 9]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.1975163715843296 uplift_hit_rate: -6.663854067155237 mrr: 0.4609974182112353 uplift_mrr: -2.468728386497536\n",
      "Train idx: [0 1 2 3 4 5 6 7 8 9], Test idx: [10 11 12 13]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.18878523085612545 uplift_hit_rate: -5.607384571937282 mrr: 0.4500056619439219 uplift_mrr: -1.4586141728638173\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Test idx: [14 15 16 17]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.2081134850303727 uplift_hit_rate: 4.0567425151863405 mrr: 0.46701370542355036 uplift_mrr: 2.265774910266509\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17], Test idx: [18 19 20 21]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.17698985806504441 uplift_hit_rate: -11.505070967477801 mrr: 0.4419719346477357 uplift_mrr: -3.2178245296929013\n",
      "Train idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21], Test idx: [22 23 24 25]\n",
      "Training CatBoost model...\n",
      "hit_rate: 0.2332073676466253 uplift_hit_rate: 2.185719733566208 mrr: 0.496386243737362 uplift_mrr: 1.3894457910508333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Start an mlflow experiment\n",
    "mlflow.set_experiment(\"Catboost_Ranker\")\n",
    "\n",
    "pos_neg_ratio = 1\n",
    "experiment_name = f\"pn_ratio_{pos_neg_ratio}_all_feats\"\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{experiment_name}\"):\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=4)\n",
    "    idx = (\n",
    "        users_df[[\"EXPERIMENT_ID\", \"EXPERIMENT_DATE\"]]\n",
    "        .sort_values(\"EXPERIMENT_DATE\")\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(idx):\n",
    "        print(f\"Train idx: {train_idx}, Test idx: {test_idx}\")\n",
    "        train_idx = idx.iloc[train_idx][\"EXPERIMENT_ID\"].values\n",
    "        test_idx = idx.iloc[test_idx][\"EXPERIMENT_ID\"].values\n",
    "\n",
    "        # prepare train data\n",
    "        train_df = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(train_idx)\n",
    "        ]\n",
    "\n",
    "        train_pool, _, X_train, y_train = get_pooled_dataset(train_df, pos_neg_ratio)\n",
    "\n",
    "        # prepare test data\n",
    "        test_df_raw = users_all_variations[\n",
    "            users_all_variations[\"EXPERIMENT_ID\"].isin(test_idx)\n",
    "        ]\n",
    "        test_df = test_df_raw.groupby([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).filter(\n",
    "            lambda g: g[\"CLICK\"].max() == 1\n",
    "        )\n",
    "\n",
    "        test_pool, test_group_ids, X_test, y_test = get_pooled_dataset(test_df)\n",
    "        cat_features = train_pool.get_cat_feature_indices()\n",
    "\n",
    "        # Train CatBoost model\n",
    "\n",
    "        print(\"Training CatBoost model...\")\n",
    "        model = get_model(\"ranker\", cat_features)\n",
    "        model.fit(train_pool)\n",
    "\n",
    "        scores = model.predict(X_test)\n",
    "        preds = test_df.sort_values([\"EXPERIMENT_ID\", \"RECIPIENT_ID\"]).assign(\n",
    "            PRED=scores, GT=y_test\n",
    "        )[[\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"PRED\"]]\n",
    "        y_true = test_df[\n",
    "            [\"EXPERIMENT_ID\", \"RECIPIENT_ID\", \"VARIATION_ID\", \"CLICK\"]\n",
    "        ].query(\"CLICK==1\")\n",
    "\n",
    "        hit_rate, uplift_hit_rate = hit_rate_at_k(preds, y_true, k=1)\n",
    "        mrr, uplift_mrr = mrr_at_k(preds, y_true, 5)\n",
    "\n",
    "        # Gather the metrics for this split. You could add more metrics if needed.\n",
    "        mlflow.log_metric(\"avg_hit_rate_at_1\", hit_rate, step=len(train_idx))\n",
    "        mlflow.log_metric(\"avg_mrr_at_5\", mrr, step=len(train_idx))\n",
    "        mlflow.log_metric(\n",
    "            \"avg_uplift_hit_rate_at_1\", uplift_hit_rate, step=len(train_idx)\n",
    "        )\n",
    "        mlflow.log_metric(\"avg_uplift_mrr_at_5\", uplift_mrr, step=len(train_idx))\n",
    "\n",
    "        print(\n",
    "            \"hit_rate:\",\n",
    "            hit_rate,\n",
    "            \"uplift_hit_rate:\",\n",
    "            uplift_hit_rate,\n",
    "            \"mrr:\",\n",
    "            mrr,\n",
    "            \"uplift_mrr:\",\n",
    "            uplift_mrr,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3c346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eikona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
