{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Training Notebook\n",
    "## Data Loading and Preprocessing for Recommendation System\n",
    "\n",
    "This notebook loads click/non-click data, merges with creative features, and applies specific filtering rules to prepare a clean dataset for CatBoost model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìä Pandas version: 2.3.3\n",
      "üî¢ NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Loading click/non-click variation data and creative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading datasets...\n",
      "============================================================\n",
      "‚úÖ Loaded clicked data: (47710, 24)\n",
      "‚ùå Loaded non-clicked data: (47700, 24)\n",
      "\n",
      "üîÑ Combined dataset shape: (95410, 24)\n",
      "üìä Total records: 95,410\n",
      "‚úÖ Click rate: 50.01%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load clicked and non-clicked datasets\n",
    "print(\"üìÅ Loading datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load clicked variations\n",
    "clicked_df = pd.read_csv('data/clicked_variation_rows.csv')\n",
    "clicked_df['CLICKED'] = 1\n",
    "print(f\"‚úÖ Loaded clicked data: {clicked_df.shape}\")\n",
    "\n",
    "# Load non-clicked variations\n",
    "non_clicked_df = pd.read_csv('data/non_clicked_variation_rows.csv')\n",
    "non_clicked_df['CLICKED'] = 0\n",
    "# Remove RN column if it exists (as seen in EDA notebook)\n",
    "if 'RN' in non_clicked_df.columns:\n",
    "    non_clicked_df = non_clicked_df.drop(columns=['RN'])\n",
    "print(f\"‚ùå Loaded non-clicked data: {non_clicked_df.shape}\")\n",
    "\n",
    "# Combine clicked and non-clicked data\n",
    "combined_df = pd.concat([clicked_df, non_clicked_df], ignore_index=True)\n",
    "print(f\"\\nüîÑ Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"üìä Total records: {len(combined_df):,}\")\n",
    "print(f\"‚úÖ Click rate: {combined_df['CLICKED'].mean():.2%}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìé Loading creative features...\n",
      "Shape: (286, 15)\n",
      "Unique variations: 286\n",
      "Columns: ['variation_id', 'image_name', 'experiment_id', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'subject_line', 'category']\n",
      "\n",
      "üîó Merging with creative features...\n",
      "‚úÖ Merged dataset shape: (95410, 39)\n",
      "üìä Total records: 95,410\n",
      "\n",
      "‚ö†Ô∏è Unmatched variations (no creative features): 654 (0.69%)\n"
     ]
    }
   ],
   "source": [
    "# Load creative features\n",
    "print(\"\\nüìé Loading creative features...\")\n",
    "creative_features_df = pd.read_csv('data/combined_feats.csv', index_col=0)\n",
    "print(f\"Shape: {creative_features_df.shape}\")\n",
    "print(f\"Unique variations: {creative_features_df['variation_id'].nunique()}\")\n",
    "print(f\"Columns: {list(creative_features_df.columns)}\")\n",
    "\n",
    "# Join creative features with combined data\n",
    "print(\"\\nüîó Merging with creative features...\")\n",
    "df = combined_df.merge(\n",
    "    creative_features_df,\n",
    "    left_on='VARIATION_ID',\n",
    "    right_on='variation_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Merged dataset shape: {df.shape}\")\n",
    "print(f\"üìä Total records: {len(df):,}\")\n",
    "\n",
    "# Check for unmatched variations\n",
    "unmatched = df['variation_id'].isna().sum()\n",
    "print(f\"\\n‚ö†Ô∏è Unmatched variations (no creative features): {unmatched:,} ({unmatched/len(df):.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Data Filtering Rules\n",
    "Applying sequential filtering to clean and prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Starting data filtering process...\n",
      "============================================================\n",
      "Initial dataset size: 95,410 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ Starting data filtering process...\")\n",
    "print(\"=\"*60)\n",
    "initial_size = len(df)\n",
    "print(f\"Initial dataset size: {initial_size:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Removing rows with null creative features...\n",
      "   Removed: 654 rows (0.69%)\n",
      "   Remaining: 94,756 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows with null creative features\n",
    "print(\"1Ô∏è‚É£ Removing rows with null creative features...\")\n",
    "before = len(df)\n",
    "df = df[df['variation_id'].notna()]\n",
    "after = len(df)\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2Ô∏è‚É£ Geographic filtering (keeping only United States)...\n",
      "   Removed: 3,088 rows (3.26%)\n",
      "   Remaining: 91,668 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Geographic Filtering\n",
    "print(\"2Ô∏è‚É£ Geographic filtering (keeping only United States)...\")\n",
    "before = len(df)\n",
    "\n",
    "# First, convert 'US' to 'United States'\n",
    "df['COUNTRY'] = df['COUNTRY'].replace('US', 'United States')\n",
    "\n",
    "# Keep only United States\n",
    "df = df[df['COUNTRY'] == 'United States']\n",
    "after = len(df)\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Ô∏è‚É£ Client type filtering (removing library/email)...\n",
      "   Removed: 27 rows (0.03%)\n",
      "   Remaining: 91,641 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Client Type Filtering\n",
    "print(\"3Ô∏è‚É£ Client type filtering (removing library/email)...\")\n",
    "before = len(df)\n",
    "\n",
    "# Remove library and email client types\n",
    "df = df[~df['LATEST_CLICK_CLIENT_TYPE'].str.lower().str.contains('library|email', na=False)]\n",
    "after = len(df)\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4Ô∏è‚É£ Client name filtering (removing android/opera)...\n",
      "   Removed: 89 rows (0.10%)\n",
      "   Remaining: 91,552 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Client Name Filtering\n",
    "print(\"4Ô∏è‚É£ Client name filtering (removing android/opera)...\")\n",
    "before = len(df)\n",
    "\n",
    "# Remove android and opera client names\n",
    "df = df[~df['LATEST_CLICK_CLIENT_NAME'].str.lower().str.contains('android|opera', na=False)]\n",
    "after = len(df)\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5Ô∏è‚É£ UTM source filtering (allowing explicit allowed sources + NaN)...\n",
      "   Allowed sources: ['klaviyo', 'fb', 'google', 'ig', 'smsbump', 'applovin', 'arp', 'bing', 'facebook', 'orderlyemails'] + NaN\n",
      "   Removed: 522 rows (0.57%)\n",
      "   Remaining: 91,030 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. UTM Source Filtering\n",
    "print(\"5Ô∏è‚É£ UTM source filtering (allowing explicit allowed sources + NaN)...\")\n",
    "before = len(df)\n",
    "\n",
    "# Define allowed UTM sources\n",
    "allowed_utm_sources = ['klaviyo', 'fb', 'google', 'ig', 'smsbump', 'applovin', 'arp', 'bing', 'facebook', 'orderlyemails']\n",
    "\n",
    "# Make mask for allowed sources (case-insensitive) or NaN\n",
    "utm_source_lower = df['FIRST_UTM_SOURCE'].str.lower()\n",
    "allowed_mask = utm_source_lower.isin([s.lower() for s in allowed_utm_sources])\n",
    "nan_mask = df['FIRST_UTM_SOURCE'].isna()\n",
    "df = df[allowed_mask | nan_mask]\n",
    "\n",
    "after = len(df)\n",
    "print(f\"   Allowed sources: {allowed_utm_sources} + NaN\")\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6Ô∏è‚É£ Experiment size filtering (removing 5 experiments with <500 clicks)...\n",
      "   Experiments to remove (click counts): [338]\n",
      "   Experiment IDs: ['e627d7f0-46c4-4894-872e-59a2fc108c30']...\n",
      "   Removed: 684 rows (0.75%)\n",
      "   Remaining: 90,346 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Experiment Size Filtering\n",
    "print(\"6Ô∏è‚É£ Experiment size filtering (removing 5 experiments with <500 clicks)...\")\n",
    "before = len(df)\n",
    "\n",
    "# Calculate clicks per experiment\n",
    "experiment_clicks = df.groupby('EXPERIMENT_ID')['CLICKED'].sum().reset_index()\n",
    "experiment_clicks.columns = ['EXPERIMENT_ID', 'total_clicks']\n",
    "\n",
    "# Find experiments with less than 500 clicks\n",
    "low_click_experiments = experiment_clicks[experiment_clicks['total_clicks'] < 500]\n",
    "low_click_experiments = low_click_experiments.nsmallest(5, 'total_clicks')\n",
    "\n",
    "print(f\"   Experiments to remove (click counts): {low_click_experiments['total_clicks'].tolist()}\")\n",
    "print(f\"   Experiment IDs: {low_click_experiments['EXPERIMENT_ID'].tolist()[:3]}...\") # Show first 3 IDs\n",
    "\n",
    "# Remove these experiments\n",
    "df = df[~df['EXPERIMENT_ID'].isin(low_click_experiments['EXPERIMENT_ID'])]\n",
    "after = len(df)\n",
    "print(f\"   Removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7Ô∏è‚É£ Creative feature filtering...\n",
      "   Messaging Approach (Q3): removed 'aesthetic appeal' - 187 rows\n",
      "\n",
      "   Total removed: 187 rows (0.21%)\n",
      "   Remaining: 90,159 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Creative Feature Filtering\n",
    "print(\"7Ô∏è‚É£ Creative feature filtering...\")\n",
    "before = len(df)\n",
    "\n",
    "# Define filters for each creative feature\n",
    "creative_filters = [\n",
    "    ('Q3', 'aesthetic appeal', 'Messaging Approach'),\n",
    "    ('Q7', 'three', 'Model Count'),\n",
    "    ('Q8', '46-65', 'Age Group')\n",
    "]\n",
    "\n",
    "for col, value, description in creative_filters:\n",
    "    before_filter = len(df)\n",
    "    # Case-insensitive comparison\n",
    "    df = df[df[col].str.lower() != value.lower()]\n",
    "    removed = before_filter - len(df)\n",
    "    if removed > 0:\n",
    "        print(f\"   {description} ({col}): removed '{value}' - {removed:,} rows\")\n",
    "\n",
    "after = len(df)\n",
    "print(f\"\\n   Total removed: {before - after:,} rows ({(before - after)/before:.2%})\")\n",
    "print(f\"   Remaining: {after:,} rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Converted FIRST_ACTIVE_TS to FIRST_ACTIVE_TS_DAYS (days from 2025-10-17)\n",
      "Sample values:\n",
      "   FIRST_ACTIVE_TS_DAYS\n",
      "0                282.00\n",
      "2                102.00\n",
      "3                108.00\n",
      "4                 28.00\n",
      "5                257.00\n"
     ]
    }
   ],
   "source": [
    "# Convert FIRST_ACTIVE_TS to days from today, replace with numeric\n",
    "\n",
    "# Today's date (corresponds to environment's current date)\n",
    "from datetime import datetime\n",
    "today = datetime(2025, 10, 17)\n",
    "\n",
    "def convert_to_days_from_today(ts_str):\n",
    "    try:\n",
    "        if pd.isna(ts_str) or ts_str == 'missing':\n",
    "            return np.nan\n",
    "        # Parse the timestamp string\n",
    "        ts = datetime.strptime(ts_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        days_diff = (today - ts).days\n",
    "        return days_diff\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "if 'FIRST_ACTIVE_TS' in df.columns:\n",
    "    df['FIRST_ACTIVE_TS_DAYS'] = df['FIRST_ACTIVE_TS'].apply(convert_to_days_from_today)\n",
    "    df = df.drop(columns=['FIRST_ACTIVE_TS'])\n",
    "\n",
    "    print(\"‚úì Converted FIRST_ACTIVE_TS to FIRST_ACTIVE_TS_DAYS (days from 2025-10-17)\")\n",
    "    print(f\"Sample values:\\n{df[['FIRST_ACTIVE_TS_DAYS']].head()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8Ô∏è‚É£ Removing  columns...\n",
      "   No timestamp columns found to drop\n",
      "\n",
      "‚úÖ Filtering complete!\n",
      "\n",
      " Dataset columns: ['EXPERIMENT_ID', 'VARIATION_ID', 'MONTH', 'DAY', 'HOUR', 'RECIPIENT_ID', 'CITY', 'COUNTRY', 'REGION', 'TIMEZONE', 'LAST_ORDER_VALUE', 'TOTAL_ORDERS_VALUE', 'AVG_ORDER_VALUE', 'LATEST_CLICK_CLIENT_TYPE', 'LATEST_CLICK_CLIENT_OS_FAMILY', 'LATEST_CLICK_CLIENT_NAME', 'FIRST_UTM_SOURCE', 'FIRST_UTM_CONTENT', 'FIRST_UTM_CAMPAIGN', 'CLICKED', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'category', 'FIRST_ACTIVE_TS_DAYS']\n"
     ]
    }
   ],
   "source": [
    "# 8. Remove columns\n",
    "print(\"8Ô∏è‚É£ Removing  columns...\")\n",
    "columns_to_drop = ['LAST_EVENT_DATE', 'LATEST_CLICK_DATE', 'subject_line', 'image_name', 'variation_id', 'CLICK_COUNT', 'experiment_id']\n",
    "existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "if existing_columns:\n",
    "    df = df.drop(columns=existing_columns)\n",
    "    print(f\"   Dropped columns: {existing_columns}\")\n",
    "else:\n",
    "    print(f\"   No timestamp columns found to drop\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Filtering complete!\")\n",
    "print(f\"\\n Dataset columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Summary & Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FINAL DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìà Size Statistics:\n",
      "   Initial dataset: 95,410 rows\n",
      "   Final dataset: 90,159 rows\n",
      "   Total removed: 5,251 rows (5.50%)\n",
      "   Retention rate: 94.50%\n",
      "\n",
      "‚úÖ Click Statistics:\n",
      "   Total clicks: 45,097\n",
      "   Total non-clicks: 45,062\n",
      "   Click rate: 50.02%\n",
      "\n",
      "üî¨ Experiment Statistics:\n",
      "   Unique experiments: 25\n",
      "   Unique variations: 121\n",
      "   Avg records per experiment: 3606\n"
     ]
    }
   ],
   "source": [
    "# Final dataset summary\n",
    "print(\"üìä FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Size Statistics:\")\n",
    "print(f\"   Initial dataset: {initial_size:,} rows\")\n",
    "print(f\"   Final dataset: {len(df):,} rows\")\n",
    "print(f\"   Total removed: {initial_size - len(df):,} rows ({(initial_size - len(df))/initial_size:.2%})\")\n",
    "print(f\"   Retention rate: {len(df)/initial_size:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Click Statistics:\")\n",
    "print(f\"   Total clicks: {df['CLICKED'].sum():,}\")\n",
    "print(f\"   Total non-clicks: {(df['CLICKED'] == 0).sum():,}\")\n",
    "print(f\"   Click rate: {df['CLICKED'].mean():.2%}\")\n",
    "\n",
    "print(f\"\\nüî¨ Experiment Statistics:\")\n",
    "print(f\"   Unique experiments: {df['EXPERIMENT_ID'].nunique()}\")\n",
    "print(f\"   Unique variations: {df['VARIATION_ID'].nunique()}\")\n",
    "print(f\"   Avg records per experiment: {len(df)/df['EXPERIMENT_ID'].nunique():.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Dataset Shape:\n",
      "   Rows: 90,159\n",
      "   Columns: 32\n",
      "\n",
      "üìã Column List:\n",
      "    1. EXPERIMENT_ID\n",
      "    2. VARIATION_ID\n",
      "    3. MONTH\n",
      "    4. DAY\n",
      "    5. HOUR\n",
      "    6. RECIPIENT_ID\n",
      "    7. CITY\n",
      "    8. COUNTRY\n",
      "    9. REGION\n",
      "   10. TIMEZONE\n",
      "   11. LAST_ORDER_VALUE\n",
      "   12. TOTAL_ORDERS_VALUE\n",
      "   13. AVG_ORDER_VALUE\n",
      "   14. LATEST_CLICK_CLIENT_TYPE\n",
      "   15. LATEST_CLICK_CLIENT_OS_FAMILY\n",
      "   16. LATEST_CLICK_CLIENT_NAME\n",
      "   17. FIRST_UTM_SOURCE\n",
      "   18. FIRST_UTM_CONTENT\n",
      "   19. FIRST_UTM_CAMPAIGN\n",
      "   20. CLICKED\n",
      "   21. Q1\n",
      "   22. Q2\n",
      "   23. Q3\n",
      "   24. Q4\n",
      "   25. Q5\n",
      "   26. Q6\n",
      "   27. Q7\n",
      "   28. Q8\n",
      "   29. Q9\n",
      "   30. Q10\n",
      "   31. category\n",
      "   32. FIRST_ACTIVE_TS_DAYS\n",
      "\n",
      "üîç Sample of Final Dataset (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXPERIMENT_ID</th>\n",
       "      <th>VARIATION_ID</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>RECIPIENT_ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>REGION</th>\n",
       "      <th>TIMEZONE</th>\n",
       "      <th>LAST_ORDER_VALUE</th>\n",
       "      <th>TOTAL_ORDERS_VALUE</th>\n",
       "      <th>AVG_ORDER_VALUE</th>\n",
       "      <th>LATEST_CLICK_CLIENT_TYPE</th>\n",
       "      <th>LATEST_CLICK_CLIENT_OS_FAMILY</th>\n",
       "      <th>LATEST_CLICK_CLIENT_NAME</th>\n",
       "      <th>FIRST_UTM_SOURCE</th>\n",
       "      <th>FIRST_UTM_CONTENT</th>\n",
       "      <th>FIRST_UTM_CAMPAIGN</th>\n",
       "      <th>CLICKED</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>category</th>\n",
       "      <th>FIRST_ACTIVE_TS_DAYS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002deaf7-331f-4b5e-866b-f6dad60e4a79</td>\n",
       "      <td>148d62bc-044f-4c55-9195-8be2d7579d9d</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>01HNP4BCBMFV98J5B420PCS38F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mobile Browser</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Mobile Safari</td>\n",
       "      <td>applovin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Underoutfit_CPP</td>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Promotional / Incentive</td>\n",
       "      <td>Aesthetic Appeal</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Studio</td>\n",
       "      <td>Two</td>\n",
       "      <td>26-35</td>\n",
       "      <td>Curvy</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Sensory / lifestyle</td>\n",
       "      <td>282.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e4b4a349-3b14-439e-946f-f716101dac69</td>\n",
       "      <td>15be91b7-6b55-4cd3-8d07-f4068b018a3a</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>01HNP8Y1JFNW4G5YV6C5Z0X4SZ</td>\n",
       "      <td>Akron</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>177.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>Mobile Browser</td>\n",
       "      <td>Android</td>\n",
       "      <td>Chrome Mobile</td>\n",
       "      <td>Klaviyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>em - just dropped 460 459 457  -  Sun Jul 6 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>Model</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Social Proof / Validation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Outdoor / Nature</td>\n",
       "      <td>One</td>\n",
       "      <td>26-35</td>\n",
       "      <td>Slim</td>\n",
       "      <td>White</td>\n",
       "      <td>Conversational tone</td>\n",
       "      <td>102.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49c33d7c-ef04-43a7-bbd0-783489c64849</td>\n",
       "      <td>2dd79b7f-a399-459e-8eb0-6baf3a22f53b</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>01HNPADZ4935GQ7DA0Q76Z7J4V</td>\n",
       "      <td>Suffolk</td>\n",
       "      <td>United States</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Browser</td>\n",
       "      <td>Windows</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Klaviyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SMS Group B - Buy more save more - Mon Jun 30 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Model</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Social Proof / Validation</td>\n",
       "      <td>Fit &amp; Support</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Bedroom / Home</td>\n",
       "      <td>One</td>\n",
       "      <td>18-25</td>\n",
       "      <td>Slim</td>\n",
       "      <td>White</td>\n",
       "      <td>Sensory / lifestyle</td>\n",
       "      <td>108.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11c49e5d-21ac-4d6d-88c3-f211562a8e07</td>\n",
       "      <td>473c90cc-b034-4585-897c-da6900a9fa73</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>01HNPBGKJ35594G1B42RZ40RRV</td>\n",
       "      <td>Frontenac</td>\n",
       "      <td>United States</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mobile Browser</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Mobile Safari</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Model</td>\n",
       "      <td>Studio</td>\n",
       "      <td>Emotional / Lifestyle</td>\n",
       "      <td>Seamlessness</td>\n",
       "      <td>Bra &amp; Underwear</td>\n",
       "      <td>Studio</td>\n",
       "      <td>One</td>\n",
       "      <td>26-35</td>\n",
       "      <td>Slim</td>\n",
       "      <td>White</td>\n",
       "      <td>Sensory / lifestyle</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>823158da-7b0a-4c19-8189-663c22a3ae38</td>\n",
       "      <td>84235e36-13c5-416d-acc0-1f3d7b1c9daf</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>01HNPF51YK5QZWG06XENZY6MDX</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>United States</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>America/Indiana/Indianapolis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Browser</td>\n",
       "      <td>OS X</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Klaviyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>em - V7B for Valentines VDay - Fri Jan 31 2025...</td>\n",
       "      <td>1</td>\n",
       "      <td>Model</td>\n",
       "      <td>Studio</td>\n",
       "      <td>Functional / Product-Focused</td>\n",
       "      <td>Aesthetic Appeal</td>\n",
       "      <td>Bra</td>\n",
       "      <td>Studio</td>\n",
       "      <td>One</td>\n",
       "      <td>26-35</td>\n",
       "      <td>Curvy</td>\n",
       "      <td>White</td>\n",
       "      <td>Sensory / lifestyle</td>\n",
       "      <td>257.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          EXPERIMENT_ID                          VARIATION_ID  \\\n",
       "0  002deaf7-331f-4b5e-866b-f6dad60e4a79  148d62bc-044f-4c55-9195-8be2d7579d9d   \n",
       "2  e4b4a349-3b14-439e-946f-f716101dac69  15be91b7-6b55-4cd3-8d07-f4068b018a3a   \n",
       "3  49c33d7c-ef04-43a7-bbd0-783489c64849  2dd79b7f-a399-459e-8eb0-6baf3a22f53b   \n",
       "4  11c49e5d-21ac-4d6d-88c3-f211562a8e07  473c90cc-b034-4585-897c-da6900a9fa73   \n",
       "5  823158da-7b0a-4c19-8189-663c22a3ae38  84235e36-13c5-416d-acc0-1f3d7b1c9daf   \n",
       "\n",
       "   MONTH  DAY  HOUR                RECIPIENT_ID       CITY        COUNTRY  \\\n",
       "0      7   28    12  01HNP4BCBMFV98J5B420PCS38F        NaN  United States   \n",
       "2      8    2    12  01HNP8Y1JFNW4G5YV6C5Z0X4SZ      Akron  United States   \n",
       "3      9    6    13  01HNPADZ4935GQ7DA0Q76Z7J4V    Suffolk  United States   \n",
       "4      9   17    14  01HNPBGKJ35594G1B42RZ40RRV  Frontenac  United States   \n",
       "5      9   27    12  01HNPF51YK5QZWG06XENZY6MDX     Carmel  United States   \n",
       "\n",
       "     REGION                      TIMEZONE  LAST_ORDER_VALUE  \\\n",
       "0       NaN               America/Chicago               NaN   \n",
       "2      Ohio              America/New_York            177.00   \n",
       "3  Virginia              America/New_York               NaN   \n",
       "4  Missouri               America/Chicago               NaN   \n",
       "5   Indiana  America/Indiana/Indianapolis               NaN   \n",
       "\n",
       "   TOTAL_ORDERS_VALUE  AVG_ORDER_VALUE LATEST_CLICK_CLIENT_TYPE  \\\n",
       "0                 NaN              NaN           Mobile Browser   \n",
       "2              177.00           177.00           Mobile Browser   \n",
       "3                 NaN              NaN                  Browser   \n",
       "4                 NaN              NaN           Mobile Browser   \n",
       "5                 NaN              NaN                  Browser   \n",
       "\n",
       "  LATEST_CLICK_CLIENT_OS_FAMILY LATEST_CLICK_CLIENT_NAME FIRST_UTM_SOURCE  \\\n",
       "0                           iOS            Mobile Safari         applovin   \n",
       "2                       Android            Chrome Mobile          Klaviyo   \n",
       "3                       Windows                   Chrome          Klaviyo   \n",
       "4                           iOS            Mobile Safari              NaN   \n",
       "5                          OS X                   Safari          Klaviyo   \n",
       "\n",
       "  FIRST_UTM_CONTENT                                 FIRST_UTM_CAMPAIGN  \\\n",
       "0               NaN                                    Underoutfit_CPP   \n",
       "2               NaN  em - just dropped 460 459 457  -  Sun Jul 6 20...   \n",
       "3               NaN  SMS Group B - Buy more save more - Mon Jun 30 ...   \n",
       "4               NaN                                                NaN   \n",
       "5               NaN  em - V7B for Valentines VDay - Fri Jan 31 2025...   \n",
       "\n",
       "   CLICKED     Q1         Q2                            Q3                Q4  \\\n",
       "0        1   Text  Lifestyle       Promotional / Incentive  Aesthetic Appeal   \n",
       "2        1  Model  Lifestyle     Social Proof / Validation           Unknown   \n",
       "3        1  Model  Lifestyle     Social Proof / Validation     Fit & Support   \n",
       "4        1  Model     Studio         Emotional / Lifestyle      Seamlessness   \n",
       "5        1  Model     Studio  Functional / Product-Focused  Aesthetic Appeal   \n",
       "\n",
       "                Q5                Q6   Q7     Q8     Q9      Q10  \\\n",
       "0              Bra            Studio  Two  26-35  Curvy  Unknown   \n",
       "2              Bra  Outdoor / Nature  One  26-35   Slim    White   \n",
       "3              Bra    Bedroom / Home  One  18-25   Slim    White   \n",
       "4  Bra & Underwear            Studio  One  26-35   Slim    White   \n",
       "5              Bra            Studio  One  26-35  Curvy    White   \n",
       "\n",
       "              category  FIRST_ACTIVE_TS_DAYS  \n",
       "0  Sensory / lifestyle                282.00  \n",
       "2  Conversational tone                102.00  \n",
       "3  Sensory / lifestyle                108.00  \n",
       "4  Sensory / lifestyle                 28.00  \n",
       "5  Sensory / lifestyle                257.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display data shape and columns\n",
    "print(\"\\nüìä Final Dataset Shape:\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nüìã Column List:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "# Show sample of the final dataset\n",
    "print(\"\\nüîç Sample of Final Dataset (first 5 rows):\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÄ Data Split (train / validation / test) based on experiment exposure order:\n",
      "   Train rows: 76,818\n",
      "   Validation rows: 8,641\n",
      "   Test rows: 4,700\n",
      "   Train experiments: 21\n",
      "   Validation experiments: 2\n",
      "   Test experiments: 2\n",
      "   Validation experiment IDs: ['823158da-7b0a-4c19-8189-663c22a3ae38', 'cd4a656f-290a-41e2-be1d-bf62ad85757d']\n",
      "   Test experiment IDs: ['78a802ae-d6cd-4f39-aecb-138668fa2607', '0ef6d2e9-7601-4df6-a215-83e6e79aa24e']\n",
      "\n",
      "üóìÔ∏è  Dates of test experiments (by earliest exposure):\n",
      "   TEST  EXPERIMENT_ID: 78a802ae-d6cd-4f39-aecb-138668fa2607  |  MONTH: 10, DAY: 2\n",
      "   TEST  EXPERIMENT_ID: 0ef6d2e9-7601-4df6-a215-83e6e79aa24e  |  MONTH: 10, DAY: 6\n",
      "\n",
      "üóìÔ∏è  Dates of validation experiments (by earliest exposure):\n",
      "   VAL   EXPERIMENT_ID: 823158da-7b0a-4c19-8189-663c22a3ae38  |  MONTH: 9, DAY: 27\n",
      "   VAL   EXPERIMENT_ID: cd4a656f-290a-41e2-be1d-bf62ad85757d  |  MONTH: 9, DAY: 29\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation, and test by saving last 2 experiments as test,\n",
    "# 2 before those as validation, and all others as train,\n",
    "# where experiments are sorted by their first exposure (MONTH, DAY) combo.\n",
    "\n",
    "# Get the (MONTH, DAY) of first exposure for each experiment\n",
    "exp_timing = (\n",
    "    df.groupby('EXPERIMENT_ID')[['MONTH', 'DAY']]\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort experiments by (MONTH, DAY)\n",
    "exp_timing_sorted = exp_timing.sort_values(['MONTH', 'DAY'], ascending=[True, True])\n",
    "sorted_experiment_ids = exp_timing_sorted['EXPERIMENT_ID'].tolist()\n",
    "\n",
    "# Defining splits\n",
    "test_experiments = sorted_experiment_ids[-2:]\n",
    "val_experiments = sorted_experiment_ids[-4:-2]\n",
    "train_experiments = sorted_experiment_ids[:-4]\n",
    "\n",
    "# Gather info for printing\n",
    "test_exp_info = exp_timing_sorted[exp_timing_sorted['EXPERIMENT_ID'].isin(test_experiments)]\n",
    "val_exp_info = exp_timing_sorted[exp_timing_sorted['EXPERIMENT_ID'].isin(val_experiments)]\n",
    "\n",
    "# Create DataFrames for each split\n",
    "df_test = df[df['EXPERIMENT_ID'].isin(test_experiments)].copy()\n",
    "df_val = df[df['EXPERIMENT_ID'].isin(val_experiments)].copy()\n",
    "df_train = df[df['EXPERIMENT_ID'].isin(train_experiments)].copy()\n",
    "\n",
    "print(f\"\\nüîÄ Data Split (train / validation / test) based on experiment exposure order:\")\n",
    "print(f\"   Train rows: {df_train.shape[0]:,}\")\n",
    "print(f\"   Validation rows: {df_val.shape[0]:,}\")\n",
    "print(f\"   Test rows: {df_test.shape[0]:,}\")\n",
    "print(f\"   Train experiments: {df_train['EXPERIMENT_ID'].nunique()}\")\n",
    "print(f\"   Validation experiments: {df_val['EXPERIMENT_ID'].nunique()}\")\n",
    "print(f\"   Test experiments: {df_test['EXPERIMENT_ID'].nunique()}\")\n",
    "print(f\"   Validation experiment IDs: {val_experiments}\")\n",
    "print(f\"   Test experiment IDs: {test_experiments}\")\n",
    "\n",
    "# Print the (MONTH, DAY) of val and test experiments\n",
    "print(\"\\nüóìÔ∏è  Dates of test experiments (by earliest exposure):\")\n",
    "for _, row in test_exp_info.iterrows():\n",
    "    print(f\"   TEST  EXPERIMENT_ID: {row['EXPERIMENT_ID']}  |  MONTH: {row['MONTH']}, DAY: {row['DAY']}\")\n",
    "\n",
    "print(\"\\nüóìÔ∏è  Dates of validation experiments (by earliest exposure):\")\n",
    "for _, row in val_exp_info.iterrows():\n",
    "    print(f\"   VAL   EXPERIMENT_ID: {row['EXPERIMENT_ID']}  |  MONTH: {row['MONTH']}, DAY: {row['DAY']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßÆ X_train shape: (76818, 31)\n",
      "üßÆ y_train shape: (76818,)\n",
      "üßÆ X_val shape: (8641, 31)\n",
      "üßÆ y_val shape: (8641,)\n",
      "üßÆ X_test shape:  (4700, 31)\n",
      "üßÆ y_test shape:  (4700,)\n",
      "\n",
      "üîπ X_train columns: ['EXPERIMENT_ID', 'VARIATION_ID', 'MONTH', 'DAY', 'HOUR', 'RECIPIENT_ID', 'CITY', 'COUNTRY', 'REGION', 'TIMEZONE', 'LAST_ORDER_VALUE', 'TOTAL_ORDERS_VALUE', 'AVG_ORDER_VALUE', 'LATEST_CLICK_CLIENT_TYPE', 'LATEST_CLICK_CLIENT_OS_FAMILY', 'LATEST_CLICK_CLIENT_NAME', 'FIRST_UTM_SOURCE', 'FIRST_UTM_CONTENT', 'FIRST_UTM_CAMPAIGN', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'category', 'FIRST_ACTIVE_TS_DAYS']\n"
     ]
    }
   ],
   "source": [
    "# Split df_train and df_test into features (X) and targets (y)\n",
    "TARGET_COL = 'CLICKED'\n",
    "\n",
    "X_train = df_train.drop(columns=[TARGET_COL])\n",
    "y_train = df_train[TARGET_COL]\n",
    "\n",
    "X_val = df_val.drop(columns=[TARGET_COL])\n",
    "y_val = df_val[TARGET_COL]\n",
    "\n",
    "X_test = df_test.drop(columns=[TARGET_COL])\n",
    "y_test = df_test[TARGET_COL]\n",
    "print(f\"\\nüßÆ X_train shape: {X_train.shape}\")\n",
    "print(f\"üßÆ y_train shape: {y_train.shape}\")\n",
    "print(f\"üßÆ X_val shape: {X_val.shape}\")\n",
    "print(f\"üßÆ y_val shape: {y_val.shape}\")\n",
    "print(f\"üßÆ X_test shape:  {X_test.shape}\")\n",
    "print(f\"üßÆ y_test shape:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nüîπ X_train columns:\", list(X_train.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dadmmsvjhu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipient feature columns:\n",
      "['EXPERIMENT_ID', 'RECIPIENT_ID', 'MONTH', 'DAY', 'HOUR', 'CITY', 'COUNTRY', 'REGION', 'TIMEZONE', 'LAST_ORDER_VALUE', 'TOTAL_ORDERS_VALUE', 'AVG_ORDER_VALUE', 'LATEST_CLICK_CLIENT_TYPE', 'LATEST_CLICK_CLIENT_OS_FAMILY', 'LATEST_CLICK_CLIENT_NAME', 'FIRST_ACTIVE_TS_DAYS', 'FIRST_UTM_SOURCE', 'FIRST_UTM_CONTENT', 'FIRST_UTM_CAMPAIGN']\n",
      "\n",
      "Variation feature columns:\n",
      "['VARIATION_ID', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'category']\n",
      "\n",
      "‚úì Total cols in df: 32, categorized: 32\n"
     ]
    }
   ],
   "source": [
    "# First, let's identify which columns are recipient features vs variation features\n",
    "# Recipient features: everything that's about the user\n",
    "recipient_cols = [\n",
    "    'EXPERIMENT_ID', 'RECIPIENT_ID', 'MONTH', 'DAY', 'HOUR',\n",
    "    'CITY', 'COUNTRY', 'REGION', 'TIMEZONE',\n",
    "    'LAST_ORDER_VALUE', 'TOTAL_ORDERS_VALUE', 'AVG_ORDER_VALUE',\n",
    "    'LATEST_CLICK_CLIENT_TYPE', 'LATEST_CLICK_CLIENT_OS_FAMILY', 'LATEST_CLICK_CLIENT_NAME',\n",
    "    'FIRST_ACTIVE_TS_DAYS', 'FIRST_UTM_SOURCE', 'FIRST_UTM_CONTENT', 'FIRST_UTM_CAMPAIGN'\n",
    "]\n",
    "\n",
    "# Variation features: creative features\n",
    "variation_cols = [\n",
    "    'VARIATION_ID', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'category'\n",
    "]\n",
    "\n",
    "print(\"Recipient feature columns:\")\n",
    "print(recipient_cols)\n",
    "print(f\"\\nVariation feature columns:\")\n",
    "print(variation_cols)\n",
    "print(f\"\\n‚úì Total cols in df: {len(df.columns)}, categorized: {len(recipient_cols) + len(variation_cols) + 1}\")  # +1 for CLICKED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recall@K Evaluation Functions\n",
    "Functions for evaluating recommendation model performance using recall@k metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def enrich_with_unseen_variations(df, recipient_cols, variation_cols, verbose=False):\n",
    "    \"\"\"\n",
    "    For each (recipient, experiment) pair in the dataset, add rows for all variations\n",
    "    they didn't see in that experiment. Each new row duplicates recipient features\n",
    "    but uses the variation features from the unseen variation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Original dataset with one row per (recipient, experiment, variation) observation\n",
    "    recipient_cols : list\n",
    "        List of column names that are recipient-specific features\n",
    "    variation_cols : list\n",
    "        List of column names that are variation-specific features (including VARIATION_ID)\n",
    "    verbose : bool\n",
    "        If True, print progress information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Enriched dataset with additional rows for unseen variations, with CLICKED=0 for new rows\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting enrichment process...\")\n",
    "        print(f\"Initial dataset size: {len(df):,} rows\")\n",
    "    \n",
    "    # Create a lookup table for variation features\n",
    "    variation_features = df[variation_cols].drop_duplicates(subset=['VARIATION_ID'])\n",
    "    if verbose:\n",
    "        print(f\"Unique variations found: {len(variation_features)}\")\n",
    "    \n",
    "    # Get all variations per experiment\n",
    "    experiment_variations = df.groupby('EXPERIMENT_ID')['VARIATION_ID'].apply(set).to_dict()\n",
    "    \n",
    "    # Process each experiment separately\n",
    "    enriched_dfs = []\n",
    "    \n",
    "    for exp_id, variations_in_exp in experiment_variations.items():\n",
    "        # Get all rows for this experiment\n",
    "        exp_df = df[df['EXPERIMENT_ID'] == exp_id].copy()\n",
    "        \n",
    "        # Get unique recipients in this experiment\n",
    "        recipients = exp_df['RECIPIENT_ID'].unique()\n",
    "        \n",
    "        # Get variation features for this experiment only\n",
    "        exp_variations = variation_features[variation_features['VARIATION_ID'].isin(variations_in_exp)]\n",
    "        \n",
    "        new_rows = []\n",
    "        \n",
    "        for recipient_id in recipients:\n",
    "            # Get the recipient's existing row(s) in this experiment\n",
    "            recipient_rows = exp_df[exp_df['RECIPIENT_ID'] == recipient_id]\n",
    "            \n",
    "            # Get the variation(s) this recipient saw\n",
    "            seen_variations = set(recipient_rows['VARIATION_ID'].values)\n",
    "            \n",
    "            # Get unseen variations\n",
    "            unseen_variations = variations_in_exp - seen_variations\n",
    "            \n",
    "            if len(unseen_variations) > 0:\n",
    "                # Take one representative row for recipient features\n",
    "                recipient_features = recipient_rows[recipient_cols].iloc[0]\n",
    "                \n",
    "                # Create a row for each unseen variation\n",
    "                for unseen_var_id in unseen_variations:\n",
    "                    var_features = exp_variations[exp_variations['VARIATION_ID'] == unseen_var_id]\n",
    "                    \n",
    "                    if len(var_features) > 0:\n",
    "                        # Combine recipient and variation features\n",
    "                        new_row = pd.concat([\n",
    "                            recipient_features,\n",
    "                            var_features.iloc[0],\n",
    "                            pd.Series({'CLICKED': 0})\n",
    "                        ])\n",
    "                        new_rows.append(new_row)\n",
    "        \n",
    "        if new_rows:\n",
    "            new_rows_df = pd.DataFrame(new_rows)\n",
    "            enriched_dfs.append(pd.concat([exp_df, new_rows_df], ignore_index=True))\n",
    "        else:\n",
    "            enriched_dfs.append(exp_df)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Experiment {exp_id[:8]}...: {len(exp_df):,} ‚Üí {len(enriched_dfs[-1]):,} rows\")\n",
    "    \n",
    "    # Combine all experiments\n",
    "    result_df = pd.concat(enriched_dfs, ignore_index=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚úì Enrichment complete!\")\n",
    "        print(f\"  Final dataset size: {len(result_df):,} rows\")\n",
    "        print(f\"  Added: {len(result_df) - len(df):,} rows\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def recall_at_k(X, y, model, k, recipient_cols, variation_cols, categorical_features=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate recall@k for a recommendation system where each recipient clicks on exactly one variation\n",
    "    per experiment. The function enriches the data with unseen variations, predicts scores,\n",
    "    and measures what proportion of recipients have their clicked variation in the top-k recommendations.\n",
    "    \n",
    "    Since each recipient clicks exactly one variation:\n",
    "    - Recall for a recipient = 1 if clicked variation is in top-k, else 0\n",
    "    - Overall recall@k = average across all recipients (proportion who were \"recalled\")\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features (without target)\n",
    "    y : pd.Series\n",
    "        Target variable (CLICKED)\n",
    "    model : trained model\n",
    "        Model with predict_proba or predict method\n",
    "    k : int\n",
    "        Number of top recommendations to consider\n",
    "    recipient_cols : list\n",
    "        List of recipient feature column names\n",
    "    variation_cols : list\n",
    "        List of variation feature column names\n",
    "    categorical_features : list, optional\n",
    "        List of categorical feature names to handle NaN values\n",
    "    verbose : bool\n",
    "        If True, print detailed progress information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Recall@k score (proportion of recipients whose clicked variation was in top-k)\n",
    "    dict\n",
    "        Dictionary with detailed metrics\n",
    "    \"\"\"\n",
    "    # Concatenate X and y\n",
    "    df_eval = X.copy()\n",
    "    df_eval['CLICKED'] = y.values\n",
    "    \n",
    "    # Enrich with unseen variations\n",
    "    df_enriched = enrich_with_unseen_variations(df_eval, recipient_cols, variation_cols, verbose=verbose)\n",
    "    \n",
    "    # Handle NaN values in categorical features for prediction\n",
    "    if categorical_features is not None:\n",
    "        for col in categorical_features:\n",
    "            if col in df_enriched.columns:\n",
    "                df_enriched[col] = df_enriched[col].fillna('missing').astype(str)\n",
    "    \n",
    "    # Split back into features and target\n",
    "    y_hat = df_enriched['CLICKED']\n",
    "    X_hat = df_enriched.drop(columns=['CLICKED'])\n",
    "    \n",
    "    # Predict probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_hat)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict(X_hat)\n",
    "    \n",
    "    df_enriched['prediction'] = y_pred_proba\n",
    "    \n",
    "    # Calculate recall@k for each (recipient, experiment) pair\n",
    "    recalls = []\n",
    "    \n",
    "    for (exp_id, recipient_id), group in df_enriched.groupby(['EXPERIMENT_ID', 'RECIPIENT_ID']):\n",
    "        # Sort by prediction score (descending)\n",
    "        group_sorted = group.sort_values('prediction', ascending=False)\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        top_k = group_sorted.head(k)\n",
    "        \n",
    "        # Check if the clicked variation is in top-k\n",
    "        # Since each recipient clicks on exactly ONE variation, recall is 1 if it's in top-k, else 0\n",
    "        clicks_in_top_k = top_k['CLICKED'].sum()\n",
    "        if clicks_in_top_k > 0:\n",
    "            recall = 1.0\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    # Average recall@k across all recipients\n",
    "    mean_recall_at_k = np.mean(recalls)\n",
    "    \n",
    "    # Additional metrics\n",
    "    metrics = {\n",
    "        'recall_at_k': mean_recall_at_k,\n",
    "        'k': k,\n",
    "        'total_recipients': len(recalls),\n",
    "        'recipients_recalled': sum(recalls),\n",
    "        'total_enriched_rows': len(df_enriched),\n",
    "        'original_rows': len(X),\n",
    "        'enrichment_ratio': len(df_enriched) / len(X)\n",
    "    }\n",
    "    \n",
    "    return mean_recall_at_k, metrics\n",
    "\n",
    "print(\"‚úì Evaluation functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost Model Training\n",
    "Training a CatBoost classifier with Logloss to predict click probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Feature categorization complete\n",
      "\n",
      "Categorical features (24):\n",
      "  - EXPERIMENT_ID\n",
      "  - VARIATION_ID\n",
      "  - RECIPIENT_ID\n",
      "  - CITY\n",
      "  - COUNTRY\n",
      "  - REGION\n",
      "  - TIMEZONE\n",
      "  - LATEST_CLICK_CLIENT_TYPE\n",
      "  - LATEST_CLICK_CLIENT_OS_FAMILY\n",
      "  - LATEST_CLICK_CLIENT_NAME\n",
      "  - FIRST_UTM_SOURCE\n",
      "  - FIRST_UTM_CONTENT\n",
      "  - FIRST_UTM_CAMPAIGN\n",
      "  - Q1\n",
      "  - Q2\n",
      "  - Q3\n",
      "  - Q4\n",
      "  - Q5\n",
      "  - Q6\n",
      "  - Q7\n",
      "  - Q8\n",
      "  - Q9\n",
      "  - Q10\n",
      "  - category\n",
      "\n",
      "Numerical features (7):\n",
      "  - MONTH\n",
      "  - DAY\n",
      "  - HOUR\n",
      "  - LAST_ORDER_VALUE\n",
      "  - TOTAL_ORDERS_VALUE\n",
      "  - AVG_ORDER_VALUE\n",
      "  - FIRST_ACTIVE_TS_DAYS\n"
     ]
    }
   ],
   "source": [
    "# Import CatBoost\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "# All string/object columns and IDs are categorical\n",
    "categorical_features = [\n",
    "    'EXPERIMENT_ID', 'VARIATION_ID', 'RECIPIENT_ID',\n",
    "    'CITY', 'COUNTRY', 'REGION', 'TIMEZONE',\n",
    "    'LATEST_CLICK_CLIENT_TYPE', 'LATEST_CLICK_CLIENT_OS_FAMILY', 'LATEST_CLICK_CLIENT_NAME',\n",
    "    'FIRST_UTM_SOURCE', 'FIRST_UTM_CONTENT', 'FIRST_UTM_CAMPAIGN',\n",
    "    'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'category'\n",
    "]\n",
    "\n",
    "# Numerical features are the remaining ones\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "print(f\"‚úì Feature categorization complete\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "    \n",
    "print(f\"\\nNumerical features ({len(numerical_features)}):\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed NaN values in categorical features\n",
      "  Converted 24 categorical columns\n",
      "\n",
      "‚úì CatBoost Pools created successfully!\n",
      "  Train pool: 76818 rows, 31 features\n",
      "  Val pool: 8641 rows, 31 features\n"
     ]
    }
   ],
   "source": [
    "# Handle NaN values in categorical features by converting to string\n",
    "# CatBoost requires categorical features to be string or int, not NaN\n",
    "\n",
    "X_train_processed = X_train.copy()\n",
    "X_val_processed = X_val.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Convert NaN to string 'missing' for categorical features\n",
    "for col in categorical_features:\n",
    "    if col in X_train_processed.columns:\n",
    "        X_train_processed[col] = X_train_processed[col].fillna('missing').astype(str)\n",
    "        X_val_processed[col] = X_val_processed[col].fillna('missing').astype(str)\n",
    "        X_test_processed[col] = X_test_processed[col].fillna('missing').astype(str)\n",
    "\n",
    "print(\"‚úì Processed NaN values in categorical features\")\n",
    "print(f\"  Converted {len(categorical_features)} categorical columns\")\n",
    "\n",
    "# Create CatBoost Pools for efficient training\n",
    "train_pool = Pool(\n",
    "    data=X_train_processed,\n",
    "    label=y_train,\n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_val_processed,\n",
    "    label=y_val,\n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì CatBoost Pools created successfully!\")\n",
    "print(f\"  Train pool: {train_pool.num_row()} rows, {train_pool.num_col()} features\")\n",
    "print(f\"  Val pool: {val_pool.num_row()} rows, {val_pool.num_col()} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting CatBoost training...\n",
      "============================================================\n",
      "0:\ttest: 0.6306449\tbest: 0.6306449 (0)\ttotal: 42ms\tremaining: 41.9s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.7950325419\n",
      "bestIteration = 43\n",
      "\n",
      "Shrink model to first 44 iterations.\n",
      "\n",
      "============================================================\n",
      "‚úì Training complete!\n",
      "Best iteration: 43\n",
      "Best validation score: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Initialize CatBoost model\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=50,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting CatBoost training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Training complete!\")\n",
    "print(f\"Best iteration: {model.best_iteration_}\")\n",
    "print(f\"Best validation score: {model.best_score_['validation']['AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üìä Evaluating on VALIDATION set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test multiple k values\n",
    "k_values = [1, 3, 5]\n",
    "val_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    recall, metrics = recall_at_k(\n",
    "        X_val_processed, y_val, model, k,\n",
    "        recipient_cols, variation_cols,\n",
    "        categorical_features=categorical_features,\n",
    "        verbose=False\n",
    "    )\n",
    "    val_results[k] = metrics\n",
    "    print(f\"\\nRecall@{k}:\")\n",
    "    print(f\"  Score: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  Recipients recalled: {metrics['recipients_recalled']:.0f} / {metrics['total_recipients']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating on TEST set...\n",
      "============================================================\n",
      "\n",
      "Recall@1:\n",
      "  Score: 0.5043 (50.43%)\n",
      "  Recipients recalled: 2370 / 4700\n",
      "\n",
      "Recall@3:\n",
      "  Score: 0.5043 (50.43%)\n",
      "  Recipients recalled: 2370 / 4700\n",
      "\n",
      "Recall@5:\n",
      "  Score: 0.5043 (50.43%)\n",
      "  Recipients recalled: 2370 / 4700\n",
      "\n",
      "Recall@7:\n",
      "  Score: 0.5043 (50.43%)\n",
      "  Recipients recalled: 2370 / 4700\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üìä Evaluating on TEST set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = {}\n",
    "k_values = [1, 3, 5, 7]\n",
    "for k in k_values:\n",
    "    recall, metrics = recall_at_k(\n",
    "        X_test_processed, y_test, model, k,\n",
    "        recipient_cols, variation_cols,\n",
    "        categorical_features=categorical_features,\n",
    "        verbose=False\n",
    "    )\n",
    "    test_results[k] = metrics\n",
    "    print(f\"\\nRecall@{k}:\")\n",
    "    print(f\"  Score: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  Recipients recalled: {metrics['recipients_recalled']:.0f} / {metrics['total_recipients']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"üìà RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<15} {'Validation':<15} {'Test':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for k in k_values:\n",
    "    val_score = val_results[k]['recall_at_k']\n",
    "    test_score = test_results[k]['recall_at_k']\n",
    "    print(f\"Recall@{k:<10} {val_score:<15.4f} {test_score:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Model training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
